{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GigiQR99/NLP-exercise/blob/main/Vectorization_NLP_Lee_wk02_RQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VECTORIZATION**\n",
        "\n",
        "### **VECTORIZE:** Convert words into numbers. Make the text understandable to the ML algorithms\n",
        "\n",
        "### **PRIOR CLASS:** Pre-processing the data, **Stemming** (extract the words' root), **Lemmatize** (consider synonims, context), **Stopwords**"
      ],
      "metadata": {
        "id": "OlxJgmwxQ1PK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Categorizing Data**\n",
        "\n",
        "\n",
        "1.   **By Structure:** Structured (excel, CSV), Semistructured (JSON, HTML) they can be modified, Unstructured (the text of a phone call), is the harder to comprehend for ML w/o loss of information.\n",
        "2.   **Based on Content:** text, image, audio, video\n",
        "\n",
        "### **Noise must be removed (Cleaning):**\n",
        "Otherwise you negatively impact the results,waste GPU, waste processing time.That noise dont contribute to the meaning & semantics of the text (punctuation, emojis, etc)\n",
        "\n",
        "## **Clean Text Data**\n",
        "\n",
        "1. Stopwords removal\n",
        "2. Tokenization: split a sentence in their words/puntuation part\n",
        "3. Stemming:\n",
        "4. Lemmatization\n",
        "\n",
        "\n",
        "### **Basic NLP libraries**: for string manipulation & pattern mantching in strings\n",
        "1. **RE** (regular expresion),\n",
        "2. **textblob (~NLTK)**\n",
        "3.  **Keras** developed after Tenserflow\n"
      ],
      "metadata": {
        "id": "dyn4Tyl-agOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPORT LIBRARIES FROM SKLEARN**"
      ],
      "metadata": {
        "id": "3A-EVq2yd3Rf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "Nl8-GbtHBPL3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "# %matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Clean Text Code**\n",
        "\n",
        "####**import re**: Imports **regular expression**\n",
        "#### ***def clean_text(sentence):*** I will pass \"sentence\" into the funtion and I will return a list of clean words. Defines the function *clean_text* that accepts the argument *sentence* which is the text string you want to clean.\n",
        "#### ***return re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()***: *([^\\s\\w]|_)* This is a ***regex pattern*** (regular expresion patern). The function takes a sentence, removes punctuation and underscores by replacing them with spaces, and then returns a list of the words in the cleaned sentenc\n",
        "\n",
        "\n",
        "*   **re.sub(r'([^\\s\\w]|_)+', ' ', sentence):** This uses the re.sub function to find and replace patterns in the input sentence\n",
        "*   r'([^\\s\\w]|_)+' is the regular expression pattern. It matches one or more characters that are not whitespace (\\s) or word characters (\\w), or are an underscore (_).\n",
        "* In essence, it targets punctuation and underscores.\n",
        "* ' ' is the replacement string. Any characters matched by the pattern are replaced with a single space. sentence is the input string where the replacements will be made.\n",
        "* ***.split():*** After the punctuation and underscores are replaced with spaces, the resulting string is split into a list of words using whitespace as the delimiter.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "w42ZHjuMeJ_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Clean Data Example**"
      ],
      "metadata": {
        "id": "qmSwxvjQvQrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text(sentence):\n",
        "    return re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()"
      ],
      "metadata": {
        "id": "3bhjiDr5HEFX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your sentence\n",
        "sentence = '''The class ordered the book from MDC Shark Pack. But the book hasn't arrived yet''' #Triple quotes allows me to put anything in between.\n",
        "\n",
        "# Call the function\n",
        "result = clean_text(sentence)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvTiVg6nHvd4",
        "outputId": "d47a90ee-fdf4-48e7-f2ee-af8c017a381f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'class', 'ordered', 'the', 'book', 'from', 'MDC', 'Shark', 'Pack', 'But', 'the', 'book', 'hasn', 't', 'arrived', 'yet']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**\"n_gram_extractor\"**\n",
        "\n",
        "###Put together **words that have similar meaning** (up to 4 words). E.g: USA = United States = UniteD estates of america = America = U.S.\n",
        "\n",
        "Function for extracting n-grams from a sentence.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "***import re***: Imports the regular expression module, as seen before, for text manipulation.\n",
        "\n",
        "***def n_gram_extractor(sentence, n):*** Defines the function n_gram_extractor that takes two arguments: sentence (the input text) and n (the desired size of the n-grams).\n",
        "\n",
        "***tokens = re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split():*** This line is similar to the clean_text function you saw earlier. It cleans the input sentence by removing punctuation and underscores and then splits it into a list of individual tokens.\n",
        "\n",
        "***for i in range(len(tokens)-n+1):*** This loop iterates through the list of tokens. The range function is set up so that the loop stops when there are no longer enough tokens left to form an n-gram of size n.\n",
        "\n",
        "***print(tokens[i:i+n]):*** Inside the loop, this line extracts a slice of the tokens list starting from index i and ending at index i+n. This slice represents an n-gram of size n. The print() function then displays this n-gram.\n",
        "\n",
        "### In essence, this **function takes a sentence, cleans it, splits it into words**, and then iterates through the words to extract and print all possible sequences of n consecutive words (n-grams).\n"
      ],
      "metadata": {
        "id": "cDq3ka08QzaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def n_gram_extractor(sentence, n):\n",
        "    tokens = re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        print(tokens[i:i+n])"
      ],
      "metadata": {
        "id": "pNTq6N7mIl99"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Exercise n_gram_extractor**\n",
        "\n",
        "***n_gram_extractor('The cute little boy is playing with the kitten.', 3):*** This line calls the n_gram_extractor function with two arguments:\n",
        "The sentence string: 'The cute little boy is playing with the kitten.'\n",
        "The desired n-gram size: 3"
      ],
      "metadata": {
        "id": "hlZ7vTW1x435"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***from nltk import ngrams***: Creates a Dictionary of Tupples of 2 words each.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "89ejUEjRymRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to see bigrams\n",
        "n_gram_extractor('The cute little boy is playing with the kitten.', 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWsv5A0PrwPt",
        "outputId": "77cab3df-13ee-4826-f435-62ec03ea9945"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cute', 'little']\n",
            "['cute', 'little', 'boy']\n",
            "['little', 'boy', 'is']\n",
            "['boy', 'is', 'playing']\n",
            "['is', 'playing', 'with']\n",
            "['playing', 'with', 'the']\n",
            "['with', 'the', 'kitten']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "list(ngrams('The cute little boy is playing with the kitten.'.split(), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_RoNy2_KD6p",
        "outputId": "b9674195-6b65-47d8-9233-968df76d2726"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'cute'),\n",
              " ('cute', 'little'),\n",
              " ('little', 'boy'),\n",
              " ('boy', 'is'),\n",
              " ('is', 'playing'),\n",
              " ('playing', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'kitten.')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(\"The cute little boy is playing with the kitten.\")\n",
        "blob.ngrams(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uaygh7KoKQ7x",
        "outputId": "ad8ba371-2c8b-455b-b48e-4626618517ea"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'cute']),\n",
              " WordList(['cute', 'little']),\n",
              " WordList(['little', 'boy']),\n",
              " WordList(['boy', 'is']),\n",
              " WordList(['is', 'playing']),\n",
              " WordList(['playing', 'with']),\n",
              " WordList(['with', 'the']),\n",
              " WordList(['the', 'kitten'])]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Import nltk:*** Imports the Natural Language Toolkit library.\n",
        "\n",
        "***nltk.download('punkt_tab')***: This attempts to download the 'punkt_tab' resource from NLTK. This resource is a **tokenizer model**. The output shows that it's downloading and unzipping the resource.\n",
        "\n",
        "***from textblob import TextBlob***: Imports the TextBlob class from the textblob library. TextBlob provides a simple API for common NLP tasks.\n",
        "\n",
        "***blob = TextBlob(\"The cute little boy is playing with the kitten.\")***: Creates a TextBlob object named blob from the input sentence.\n",
        "\n",
        "***blob.ngrams(n=2)***: This calls the ngrams method on the blob object.\n",
        "n=2 specifies that you want to extract bigrams (sequences of 2 words).\n",
        "\n",
        "The output you see:\n",
        "\n",
        "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
        "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
        "\n",
        "[WordList(['The', 'cute']),\n",
        " WordList(['cute', 'little']),\n",
        " WordList(['little', 'boy']),\n",
        " WordList(['boy', 'is']),\n",
        " WordList(['is', 'playing']),\n",
        "\n",
        "Shows the bigrams extracted by TextBlob. Notice that ***TextBlob keeps punctuation attached to the last word (kitten.)*** unlike the previous n_gram_extractor function which removed punctuation first."
      ],
      "metadata": {
        "id": "1Zty0qmCzdFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pip List**  | ***!Pip list***\n",
        "### List of python packages that ve been store\n",
        "\n",
        "### The **\"!\"** at the beginning of **!pip** is **specific to Jupyter notebooks** or environments like **Google Colab.**"
      ],
      "metadata": {
        "id": "Rtwr0Wnx0Wm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip\n",
        "!pip list | grep nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys3V7wzhKxus",
        "outputId": "4d20be3e-1eab-473c-ad86-246a44a8a66a",
        "collapsed": true
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 <command> [options]\n",
            "\n",
            "Commands:\n",
            "  install                     Install packages.\n",
            "  download                    Download packages.\n",
            "  uninstall                   Uninstall packages.\n",
            "  freeze                      Output installed packages in requirements format.\n",
            "  inspect                     Inspect the python environment.\n",
            "  list                        List installed packages.\n",
            "  show                        Show information about installed packages.\n",
            "  check                       Verify installed packages have compatible dependencies.\n",
            "  config                      Manage local and global configuration.\n",
            "  search                      Search PyPI for packages.\n",
            "  cache                       Inspect and manage pip's wheel cache.\n",
            "  index                       Inspect information available from package indexes.\n",
            "  wheel                       Build wheels from your requirements.\n",
            "  hash                        Compute hashes of package archives.\n",
            "  completion                  A helper command used for command completion.\n",
            "  debug                       Show information useful for debugging.\n",
            "  help                        Show help for commands.\n",
            "\n",
            "General Options:\n",
            "  -h, --help                  Show help.\n",
            "  --debug                     Let unhandled exceptions propagate outside the\n",
            "                              main subroutine, instead of logging them to\n",
            "                              stderr.\n",
            "  --isolated                  Run pip in an isolated mode, ignoring\n",
            "                              environment variables and user configuration.\n",
            "  --require-virtualenv        Allow pip to only run in a virtual environment;\n",
            "                              exit with an error otherwise.\n",
            "  --python <python>           Run pip with the specified Python interpreter.\n",
            "  -v, --verbose               Give more output. Option is additive, and can be\n",
            "                              used up to 3 times.\n",
            "  -V, --version               Show version and exit.\n",
            "  -q, --quiet                 Give less output. Option is additive, and can be\n",
            "                              used up to 3 times (corresponding to WARNING,\n",
            "                              ERROR, and CRITICAL logging levels).\n",
            "  --log <path>                Path to a verbose appending log.\n",
            "  --no-input                  Disable prompting for input.\n",
            "  --keyring-provider <keyring_provider>\n",
            "                              Enable the credential lookup via the keyring\n",
            "                              library if user input is allowed. Specify which\n",
            "                              mechanism to use [disabled, import, subprocess].\n",
            "                              (default: disabled)\n",
            "  --proxy <proxy>             Specify a proxy in the form\n",
            "                              scheme://[user:passwd@]proxy.server:port.\n",
            "  --retries <retries>         Maximum number of retries each connection should\n",
            "                              attempt (default 5 times).\n",
            "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
            "  --exists-action <action>    Default action when a path already exists:\n",
            "                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n",
            "  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n",
            "                              even though it does not have valid or any HTTPS.\n",
            "  --cert <path>               Path to PEM-encoded CA certificate bundle. If\n",
            "                              provided, overrides the default. See 'SSL\n",
            "                              Certificate Verification' in pip documentation\n",
            "                              for more information.\n",
            "  --client-cert <path>        Path to SSL client certificate, a single file\n",
            "                              containing the private key and the certificate\n",
            "                              in PEM format.\n",
            "  --cache-dir <dir>           Store the cache data in <dir>.\n",
            "  --no-cache-dir              Disable the cache.\n",
            "  --disable-pip-version-check\n",
            "                              Don't periodically check PyPI to determine\n",
            "                              whether a new version of pip is available for\n",
            "                              download. Implied with --no-index.\n",
            "  --no-color                  Suppress colored output.\n",
            "  --no-python-version-warning\n",
            "                              Silence deprecation warnings for upcoming\n",
            "                              unsupported Pythons.\n",
            "  --use-feature <feature>     Enable new functionality, that may be backward\n",
            "                              incompatible.\n",
            "  --use-deprecated <feature>  Enable deprecated functionality, that will be\n",
            "                              removed in the future.\n",
            "nltk                                  3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**KERAS & TEXTBLOB**\n",
        "\n",
        "**Keras**: Help ot clean this text than has some typos\n",
        "\n",
        "**KERAS TOKENIZATION** does **aggresive cleaning**, **remove all special punctuation/character**, lowercases all, split on spaces\n",
        "Keras is ideal to use in tokeninzation when training NNW and normalized data.\n",
        "\n",
        "***from tensorflow.keras.preprocessing.text import text_to_word_sequence***: This **imports the text_to_word_sequence function** specifically from the Keras deep learning library (as part of TensorFlow). This is designed to tokenize text, particularly for use in neural network models.\n",
        "\n",
        "**TEXTBLOB**: Ideal when you **need more control** ver subsequent processing. You want to preserve characters likes '#' in IG, sentiment analysis (preserve emoticons), when cases sensitivity matters (emails)."
      ],
      "metadata": {
        "id": "n25ArYeY0uwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from textblob import TextBlob\n",
        "\n",
        "sentence = 'Carlos posted, \"Watching Super Bowl LVIII from Hard Rock Stadium, \\\n",
        "Miami Gardens. Amazing halftimeby Shakira! Incredible atmosphere! @miamidolphins \\\n",
        "@nfl #Miami #SuperBowl2024. For event photos contact me carlos@miamishots.com :)\"'"
      ],
      "metadata": {
        "id": "LJCPiYu_LFtE"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here Miami Dophins is considered a ***\"one word\"*** only with Keras. But hard rock stadium, Miami Gardens are still separated"
      ],
      "metadata": {
        "id": "ZnN77FV-3Sjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keras_tokens(text):\n",
        "    return text_to_word_sequence(text)\n",
        "\n",
        "# Run this to see Keras output\n",
        "result_keras = get_keras_tokens(sentence)\n",
        "print(result_keras)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJlWVnwtMGKd",
        "outputId": "497e187c-bf27-48fd-99de-d97b284ec544"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['carlos', 'posted', 'watching', 'super', 'bowl', 'lviii', 'from', 'hard', 'rock', 'stadium', 'miami', 'gardens', 'amazing', 'halftimeby', 'shakira', 'incredible', 'atmosphere', 'miamidolphins', 'nfl', 'miami', 'superbowl2024', 'for', 'event', 'photos', 'contact', 'me', 'carlos', 'miamishots', 'com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_textblob_tokens(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.words\n",
        "\n",
        "# Run this to see TextBlob output\n",
        "result_textblob = get_textblob_tokens(sentence)\n",
        "print(result_textblob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuV6r3bUMWZK",
        "outputId": "ee346bc2-0e56-4bd8-9dd9-c1901cd4bd95"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'posted', 'Watching', 'Super', 'Bowl', 'LVIII', 'from', 'Hard', 'Rock', 'Stadium', 'Miami', 'Gardens', 'Amazing', 'halftimeby', 'Shakira', 'Incredible', 'atmosphere', 'miamidolphins', 'nfl', 'Miami', 'SuperBowl2024', 'For', 'event', 'photos', 'contact', 'me', 'carlos', 'miamishots.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to see both outputs side by side\n",
        "sentence = 'Carlos posted, \"Watching Super Bowl LVIII from Hard Rock Stadium, \\\n",
        "Miami Gardens. Amazing halftimeby Shakira! Incredible atmosphere! @miamidolphins \\\n",
        "@nfl #Miami #SuperBowl2024. For event photos contact me carlos@miamishots.com :)\"'\n",
        "\n",
        "print(\"KERAS TOKENS:\")\n",
        "keras_tokens = get_keras_tokens(sentence)\n",
        "print(keras_tokens)\n",
        "print(f\"Token count: {len(keras_tokens)}\")\n",
        "\n",
        "print(\"\\nTEXTBLOB TOKENS:\")\n",
        "textblob_tokens = get_textblob_tokens(sentence)\n",
        "print(list(textblob_tokens))\n",
        "print(f\"Token count: {len(textblob_tokens)}\")\n",
        "\n",
        "print(\"\\nDIFFERENCES:\")\n",
        "print(f\"Email handling: Keras splits it, TextBlob keeps it whole\")\n",
        "print(f\"Hashtags: Keras removes #, TextBlob preserves #\")\n",
        "print(f\"Mentions: Keras removes @, TextBlob preserves @\")\n",
        "print(f\"Case: Keras lowercases, TextBlob preserves case\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1ZrfL7hMnh-",
        "outputId": "fad6fcd9-742d-4afb-e03a-2cecf2550aaa"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KERAS TOKENS:\n",
            "['carlos', 'posted', 'watching', 'super', 'bowl', 'lviii', 'from', 'hard', 'rock', 'stadium', 'miami', 'gardens', 'amazing', 'halftimeby', 'shakira', 'incredible', 'atmosphere', 'miamidolphins', 'nfl', 'miami', 'superbowl2024', 'for', 'event', 'photos', 'contact', 'me', 'carlos', 'miamishots', 'com']\n",
            "Token count: 29\n",
            "\n",
            "TEXTBLOB TOKENS:\n",
            "['Carlos', 'posted', 'Watching', 'Super', 'Bowl', 'LVIII', 'from', 'Hard', 'Rock', 'Stadium', 'Miami', 'Gardens', 'Amazing', 'halftimeby', 'Shakira', 'Incredible', 'atmosphere', 'miamidolphins', 'nfl', 'Miami', 'SuperBowl2024', 'For', 'event', 'photos', 'contact', 'me', 'carlos', 'miamishots.com']\n",
            "Token count: 28\n",
            "\n",
            "DIFFERENCES:\n",
            "Email handling: Keras splits it, TextBlob keeps it whole\n",
            "Hashtags: Keras removes #, TextBlob preserves #\n",
            "Mentions: Keras removes @, TextBlob preserves @\n",
            "Case: Keras lowercases, TextBlob preserves case\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Analyzing Miami tourism tweets\n",
        "miami_tweet = \"Just visited @ViscayaMuseum! Beautiful gardens 🌺 #MiamiDade #ArtDeco. Book tours at info@viscaya.org\"\n",
        "\n",
        "print(\"For hashtag analysis (use TextBlob):\")\n",
        "print(get_textblob_tokens(miami_tweet))  # Preserves #MiamiDade, #ArtDeco\n",
        "\n",
        "print(\"\\nFor word frequency analysis (use Keras):\")\n",
        "print(get_keras_tokens(miami_tweet))  # Normalizes everything for counting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch2o_o_HM4yX",
        "outputId": "42615b63-1cfa-4d7d-e750-51fada613bcb"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For hashtag analysis (use TextBlob):\n",
            "['Just', 'visited', 'ViscayaMuseum', 'Beautiful', 'gardens', '🌺', 'MiamiDade', 'ArtDeco', 'Book', 'tours', 'at', 'info', 'viscaya.org']\n",
            "\n",
            "For word frequency analysis (use Keras):\n",
            "['just', 'visited', 'viscayamuseum', 'beautiful', 'gardens', '🌺', 'miamidade', 'artdeco', 'book', 'tours', 'at', 'info', 'viscaya', 'org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "sentence = 'Carlos tweeted, \"Experiencing Ultra Music Festival from Bayfront Park, \\\n",
        "Miami. Incredible performance by Swedish House Mafia! Amazing visuals! @ultra \\\n",
        "@marshmello #Miami #UltraFest2024. For VIP tickets contact carlos@ultramiami.com :)\"'"
      ],
      "metadata": {
        "id": "sRgOWw9ZOZYa"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_tweet_tokenizer(text):\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    return tweet_tokenizer.tokenize(text)\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_tweet_tokenizer(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qQUGzrrOnTz",
        "outputId": "23b70b85-070c-47fa-98ff-aa89f931098a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted', ',', '\"', 'Experiencing', 'Ultra', 'Music', 'Festival', 'from', 'Bayfront', 'Park', ',', 'Miami', '.', 'Incredible', 'performance', 'by', 'Swedish', 'House', 'Mafia', '!', 'Amazing', 'visuals', '!', '@ultra', '@marshmello', '#Miami', '#UltraFest2024', '.', 'For', 'VIP', 'tickets', 'contact', 'carlos@ultramiami.com', ':)', '\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_mwe(text):\n",
        "    mwe_tokenizer = MWETokenizer([('Bayfront', 'Park')])  # Define multi-word expressions\n",
        "    mwe_tokenizer.add_mwe(('Swedish', 'House', 'Mafia!'))  # Add another MWE\n",
        "    mwe_tokenizer.add_mwe(('Ultra', 'Music', 'Festival'))\n",
        "    return mwe_tokenizer.tokenize(text.split())\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_mwe(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjZSZgaVO8qT",
        "outputId": "63151c9a-1b3b-4c35-a640-d3c4fbdbfb53"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted,', '\"Experiencing', 'Ultra_Music_Festival', 'from', 'Bayfront', 'Park,', 'Miami.', 'Incredible', 'performance', 'by', 'Swedish_House_Mafia!', 'Amazing', 'visuals!', '@ultra', '@marshmello', '#Miami', '#UltraFest2024.', 'For', 'VIP', 'tickets', 'contact', 'carlos@ultramiami.com', ':)\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_regex_tokenizer(text):\n",
        "    # Pattern: \\w+ (words) | \\$[\\d\\.]+ (prices) | \\S+ (non-spaces)\n",
        "    reg_tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "    return reg_tokenizer.tokenize(text)\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_regex_tokenizer(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zeug5pGPZxo",
        "outputId": "88516b04-1787-42ba-89b7-535c73db27a9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted', ',', '\"Experiencing', 'Ultra', 'Music', 'Festival', 'from', 'Bayfront', 'Park', ',', 'Miami', '.', 'Incredible', 'performance', 'by', 'Swedish', 'House', 'Mafia', '!', 'Amazing', 'visuals', '!', '@ultra', '@marshmello', '#Miami', '#UltraFest2024.', 'For', 'VIP', 'tickets', 'contact', 'carlos', '@ultramiami.com', ':)\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_wst(text):\n",
        "    wh_tokenizer = WhitespaceTokenizer()\n",
        "    return wh_tokenizer.tokenize(text)\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_wst(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7PdhhroQd2S",
        "outputId": "bdddd67c-4e9d-47b9-814b-1092eb77b3a4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted,', '\"Experiencing', 'Ultra', 'Music', 'Festival', 'from', 'Bayfront', 'Park,', 'Miami.', 'Incredible', 'performance', 'by', 'Swedish', 'House', 'Mafia!', 'Amazing', 'visuals!', '@ultra', '@marshmello', '#Miami', '#UltraFest2024.', 'For', 'VIP', 'tickets', 'contact', 'carlos@ultramiami.com', ':)\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_wordpunct_tokenizer(text):\n",
        "    wp_tokenizer = WordPunctTokenizer()\n",
        "    return wp_tokenizer.tokenize(text)\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_wordpunct_tokenizer(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoZq_WXcQzVT",
        "outputId": "e301e178-4ef6-4a1f-f35a-75dbb59165fb"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted', ',', '\"', 'Experiencing', 'Ultra', 'Music', 'Festival', 'from', 'Bayfront', 'Park', ',', 'Miami', '.', 'Incredible', 'performance', 'by', 'Swedish', 'House', 'Mafia', '!', 'Amazing', 'visuals', '!', '@', 'ultra', '@', 'marshmello', '#', 'Miami', '#', 'UltraFest2024', '.', 'For', 'VIP', 'tickets', 'contact', 'carlos', '@', 'ultramiami', '.', 'com', ':)\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing Miami restaurant reviews with different needs\n",
        "review = \"Amazing dinner @JoesStone! Stone crabs = $49.95. Must-try! #MiamiFood :)\"\n",
        "\n",
        "# For sentiment with emoticons\n",
        "print(\"For sentiment analysis:\")\n",
        "print(tokenize_with_tweet_tokenizer(review))\n",
        "\n",
        "# For price extraction\n",
        "print(\"\\nFor price extraction:\")\n",
        "price_tokenizer = RegexpTokenizer('\\$[\\d\\.]+|\\w+|\\S+')\n",
        "print(price_tokenizer.tokenize(review))\n",
        "\n",
        "# For keeping restaurant names\n",
        "print(\"\\nFor entity recognition:\")\n",
        "mwe = MWETokenizer([('Stone', 'crabs')])\n",
        "print(mwe.tokenize(review.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUW7hqCoQ6k4",
        "outputId": "6b20be65-0819-4110-ab32-dae9442b7934"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For sentiment analysis:\n",
            "['Amazing', 'dinner', '@JoesStone', '!', 'Stone', 'crabs', '=', '$', '49.95', '.', 'Must-try', '!', '#MiamiFood', ':)']\n",
            "\n",
            "For price extraction:\n",
            "['Amazing', 'dinner', '@JoesStone!', 'Stone', 'crabs', '=', '$49.95.', 'Must', '-try!', '#MiamiFood', ':)']\n",
            "\n",
            "For entity recognition:\n",
            "['Amazing', 'dinner', '@JoesStone!', 'Stone_crabs', '=', '$49.95.', 'Must-try!', '#MiamiFood', ':)']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:10: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-535133730.py:10: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  price_tokenizer = RegexpTokenizer('\\$[\\d\\.]+|\\w+|\\S+')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "def get_stems(text):\n",
        "    regex_stemmer = RegexpStemmer('ing$', min=4) # creating an object of RegexpStemmer,\n",
        "                                             # any string ending with the given\n",
        "                                             # regex ‘ing$’ will be removed.\n",
        "    # The below code line will convert every word into its stem using regex stemmer\n",
        "    # and then join them with space.\n",
        "    return ' '.join([regex_stemmer.stem(wd) for wd in text.split()])\n",
        "\n",
        "\n",
        "sentence = \"I love playing football\"\n",
        "get_stems(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dQhsp4I4Sqo-",
        "outputId": "95e2279a-33aa-488d-95a2-c1356d46b264"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I love play football'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "sentence = \"Before eating it would be nice to sanitize your hands with a sanitizer\"\n",
        "\n",
        "def get_stems(text):\n",
        "    ps_stemmer = PorterStemmer()\n",
        "    return ' '.join([ps_stemmer.stem(wd) for wd in text.split()])\n",
        "\n",
        "get_stems(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pzWPq_rtS1Nf",
        "outputId": "697d2177-2000-443b-98fe-1f93638b79a1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'befor eat it would be nice to sanit your hand with a sanit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "sentence = \"The products produced by the process today are far better than what it produces generally.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2tOqwCUUH_t",
        "outputId": "3a27c475-9e4c-4f68-e236-b777dfd9e396"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_lemmas(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
        "\n",
        "\n",
        "\n",
        "get_lemmas(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1fmu67xmUR4b",
        "outputId": "7ac2341f-7ab7-4df1-c5e0-b818533d20db"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The product produced by the process today are far better than what it produce generally .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "sentence = TextBlob('She sells seashells on the seashore')"
      ],
      "metadata": {
        "id": "vdv_Dy41UfiB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBkBGOhnUhyJ",
        "outputId": "8836d26b-3006-4419-f69c-23559713378b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['She', 'sells', 'seashells', 'on', 'the', 'seashore'])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def singularize(word):\n",
        "    return word.singularize()\n",
        "\n",
        "singularize(sentence.words[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9InZMhuFUj8J",
        "outputId": "c507f307-4300-4786-b552-4ad9a50dde70"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'seashell'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pluralize(word):\n",
        "    return word.pluralize()\n",
        "\n",
        "pluralize(sentence.words[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "61dHSIgvUq_W",
        "outputId": "d9b7c939-ef4c-42a0-ea3e-aa5b508144a4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'seashores'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c895f1c",
        "outputId": "a2439709-f5e8-40b1-b5ee-8943af4ada2a"
      },
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***!pip install textblob[translate]*** = not available in textblob\n",
        "\n",
        "This **install additional dependencies required for translation within textblob**. The ! at the beginning signifies that this is a shell command being executed directly in the notebook jupyter/collab environment.\n",
        "\n",
        "The **output** shows that the requirements are already satisfied, meaning the necessary packages are already installed. It also ***shows a warning that the textblob version 0.19.0 does not provide the extra 'translate'***, which might mean the translation functionality might not be available as expected.\n",
        "\n"
      ],
      "metadata": {
        "id": "fDB0RXIKEEAW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0667d52a",
        "outputId": "69ea9550-2bcf-4ea4-f8df-ae112e92cc0b"
      },
      "source": [
        "!pip install textblob[translate]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob[translate] in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "\u001b[33mWARNING: textblob 0.19.0 does not provide the extra 'translate'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob[translate]) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob[translate]) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob[translate]) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob[translate]) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob[translate]) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ALTERNATIVE: Use Google translate**"
      ],
      "metadata": {
        "id": "h4pWv7roFN5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================\n",
        "# AUSE GOOGLETRANS\n",
        "#===============================\n",
        "\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "def translate_alt(text, from_l, to_l):\n",
        "    translator = Translator()\n",
        "    result = translator.translate(text, src=from_l, dest=to_l)\n",
        "    return result.text\n",
        "\n",
        "# Test\n",
        "result = translate_alt(text='por favor', from_l='es', to_l='en')\n",
        "print(result)  # Should output: \"Please\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lUedWwdxU4I7",
        "outputId": "844398d9-eea8-4156-848d-9e1994282be4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.8.3)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=6b34321919025732b8c44697c30493791484aa244dd7200b3ad03907963737b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/0f/04/b17a72024b56a60e499ce1a6313d283ed5ba332407155bee03\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.3.0\n",
            "    Uninstalling h2-4.3.0:\n",
            "      Successfully uninstalled h2-4.3.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langsmith 0.4.28 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 1.13.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio 5.46.0 requires httpx<1.0,>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "mcp 1.14.1 requires httpx>=0.27.1, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.38.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.108.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "94294d7764474b85923521c498895f38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "please\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Must restart the session to obtain the proprer google translation**\n",
        "\n",
        "The warning means that some parts of the \"chardet\" and \"idna\" libraries were already loaded into the Colab environment before you installed the specific version of googletrans that required different versions of those libraries. To ensure that the newly installed versions of chardet and idna are used by all libraries, you need to restart the Colab runtime.\n",
        "\n",
        "It's like needing to restart your computer after installing some software updates for them to take full effect.\n"
      ],
      "metadata": {
        "id": "ZwQ6aAxyF182"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ▶ **WHAT IS SpaCy?**: ❗Review this!!\n",
        "### https://spacy.io/\n",
        "\n",
        "1. It is a free open-license  industrial-strengh NLP\n",
        "2. The spacy-llm package **integrates LLMs** into spaCy, featuring a modular system for **fast prototyping & prompting**, and turning unstructured responses into robust outputs for various NLP tasks, no training data required.\n",
        "3. If you **add spaCy as conector to your LLM** (Claude ideally) it provides de code for **automatic text cleaning**.\n",
        "\n",
        "**Video NLP class 2**, time 2:00hrs, Dr. Lee explains hot to add Spacy.io to Claude\n",
        "\n",
        "### **Steps to use SpaCy repository:**\n",
        "1. Copy its repository from **github**: https://github.com/explosion/spaCy\n",
        "2. go to https://gitmcp.io/\n",
        "3. Paste it in the **gitmcp** ***\"try example\"*** blank space, and click on **\"to MCP\"**.\n",
        "\n",
        "MCP server is a backend component that serves tools and resources to an AI system (the MCP client) through the Model Context Protocol (MCP). It acts as an intermediary, handling complex tasks like database operations. a backend component that serves tools and resources to an AI system (the MCP client) through the Model Context Protocol (MCP).\n",
        "\n",
        "4. It will convert into an MCP server inmediately, and then copy paste that MCP server URL (https://gitmcp.io/explosion/spaCy)\n",
        "\n",
        "5. Open Claude, and **\"+ add as connector\"** > click on **\"Manage connector\"**\n",
        "6. Scroll down and  click on **\"Add custom connector\"** = set a name e.g : \"Spacey documentation\"\n",
        "\n",
        "So whenever you have a question regarding SpaCy programming NLP, you can use diretly this in the prompt to Claude\n",
        "\n",
        "***\"please give me a small SPacy proof of concept that will take a pragraph ad clean it. use mcp\"***\n",
        "\n",
        "✊ **ALL THE 307 LINES OF CODE BELOW WAS GENERATED WITH SPACY, with no error!!**\n",
        "\n"
      ],
      "metadata": {
        "id": "6mHOJGTPGfKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXAMPLE OF SPACY CODE THROUGH CLAUDE\n",
        "\n",
        "This code uses the spaCy library to clean and analyze text. It defines a class called TextCleaner which has tools to:\n",
        "\n",
        "**Load a spaCy language model.**\n",
        "1. Clean text by ***removing things like punctuation, extra spaces, stop words*** (common words like 'the', 'a'), ****numbers, emails, and website addresses***.\n",
        "\n",
        "2. It can also **convert text to lowercase** and **find the base form of words** (lemmatization).\n",
        "\n",
        "3. **Find & list specific types of information** in the text, like names of people, organizations, or dates (***NER: extracting entities***).\n",
        "\n",
        "4. **Identify the grammatical role of each word** (like noun, verb, adjective)\n",
        "\n",
        "5. **Remove words based on their grammatical role**.\n",
        "\n",
        "The main part of the code shows how to **use the TextCleaner** by applying these ***cleaning and analysis steps*** to a sample paragraph and printing the results."
      ],
      "metadata": {
        "id": "HgRwfy9VPQ7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "#=====================================\n",
        "# SpaCy Text Cleaning Proof of Concept\n",
        "#=====================================\n",
        "# A comprehensive example showing various text cleaning techniques using SpaCy.\n",
        "\n",
        "\"\"\"\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import re\n",
        "from typing import List, Set\n",
        "\n",
        "class TextCleaner:\n",
        "    \"\"\"A comprehensive text cleaning pipeline using SpaCy.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
        "        \"\"\"\n",
        "        Initialize the text cleaner with a SpaCy model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the SpaCy model to load\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.nlp = spacy.load(model_name)\n",
        "        except OSError:\n",
        "            print(f\"Model '{model_name}' not found. Installing...\")\n",
        "            import subprocess\n",
        "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", model_name])\n",
        "            self.nlp = spacy.load(model_name)\n",
        "\n",
        "        # Get stop words from SpaCy\n",
        "        self.stop_words = STOP_WORDS\n",
        "\n",
        "    def clean_text(self,\n",
        "                   text: str,\n",
        "                   lowercase: bool = True,\n",
        "                   remove_stopwords: bool = True,\n",
        "                   remove_punctuation: bool = True,\n",
        "                   remove_numbers: bool = False,\n",
        "                   remove_spaces: bool = True,\n",
        "                   lemmatize: bool = True,\n",
        "                   remove_emails: bool = True,\n",
        "                   remove_urls: bool = True,\n",
        "                   remove_special_chars: bool = True,\n",
        "                   min_token_length: int = 2) -> str:\n",
        "        \"\"\"\n",
        "        Clean text using various techniques.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to clean\n",
        "            lowercase: Convert to lowercase\n",
        "            remove_stopwords: Remove stop words\n",
        "            remove_punctuation: Remove punctuation\n",
        "            remove_numbers: Remove numeric tokens\n",
        "            remove_spaces: Remove extra whitespace\n",
        "            lemmatize: Convert words to lemmas\n",
        "            remove_emails: Remove email addresses\n",
        "            remove_urls: Remove URLs\n",
        "            remove_special_chars: Remove special characters\n",
        "            min_token_length: Minimum token length to keep\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "\n",
        "        # Pre-processing: Remove URLs and emails using regex\n",
        "        if remove_urls:\n",
        "            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "            text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "        if remove_emails:\n",
        "            text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Process with SpaCy\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Token-level cleaning\n",
        "        cleaned_tokens = []\n",
        "\n",
        "        for token in doc:\n",
        "            # Skip based on various criteria\n",
        "            if remove_punctuation and token.is_punct:\n",
        "                continue\n",
        "\n",
        "            if remove_stopwords and token.text.lower() in self.stop_words:\n",
        "                continue\n",
        "\n",
        "            if remove_numbers and (token.like_num or token.is_digit):\n",
        "                continue\n",
        "\n",
        "            if token.is_space:\n",
        "                continue\n",
        "\n",
        "            # Get the token text (lemma or original)\n",
        "            if lemmatize and not token.is_punct:\n",
        "                token_text = token.lemma_\n",
        "            else:\n",
        "                token_text = token.text\n",
        "\n",
        "            # Apply lowercase\n",
        "            if lowercase:\n",
        "                token_text = token_text.lower()\n",
        "\n",
        "            # Check minimum length\n",
        "            if len(token_text) < min_token_length:\n",
        "                continue\n",
        "\n",
        "            # Remove special characters if requested\n",
        "            if remove_special_chars:\n",
        "                token_text = re.sub(r'[^a-zA-Z0-9\\s]', '', token_text)\n",
        "\n",
        "            # Add to cleaned tokens if not empty\n",
        "            if token_text.strip():\n",
        "                cleaned_tokens.append(token_text)\n",
        "\n",
        "        # Join tokens\n",
        "        cleaned_text = ' '.join(cleaned_tokens)\n",
        "\n",
        "        # Remove extra spaces\n",
        "        if remove_spaces:\n",
        "            cleaned_text = ' '.join(cleaned_text.split())\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "    def extract_entities(self, text: str) -> List[tuple]:\n",
        "        \"\"\"\n",
        "        Extract named entities from text.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            List of (entity_text, entity_label) tuples\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    def get_pos_tags(self, text: str) -> List[tuple]:\n",
        "        \"\"\"\n",
        "        Get part-of-speech tags for tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            List of (token, pos_tag) tuples\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    def remove_by_pos(self, text: str, pos_to_remove: Set[str]) -> str:\n",
        "        \"\"\"\n",
        "        Remove tokens based on their POS tags.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            pos_to_remove: Set of POS tags to remove (e.g., {'ADV', 'ADJ'})\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        tokens = [token.text for token in doc if token.pos_ not in pos_to_remove]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def normalize_whitespace(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize various types of whitespace characters.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            Text with normalized whitespace\n",
        "        \"\"\"\n",
        "        # Replace various whitespace characters with regular space\n",
        "        text = re.sub(r'[\\t\\n\\r\\f\\v]+', ' ', text)\n",
        "        # Remove multiple spaces\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Demonstrate the text cleaning capabilities.\"\"\"\n",
        "\n",
        "    # Sample paragraph with various issues\n",
        "    sample_text = \"\"\"\n",
        "    Hello World!!!  This is a SAMPLE paragraph with     various issues.\n",
        "    Visit our website at https://www.example.com or contact us at info@example.com.\n",
        "\n",
        "    The meeting is scheduled for 15th December, 2024 at 3:30 PM.\n",
        "    We'll be discussing the Q4 2024 results and planning for 2025.\n",
        "\n",
        "    Some unnecessary words: very, really, actually, basically are often overused.\n",
        "    Numbers like 123, 456.78 and special characters @#$% should be handled!\n",
        "\n",
        "    Microsoft Corporation (MSFT) announced new features.   Apple Inc. is also innovating.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SPACY TEXT CLEANING PROOF OF CONCEPT\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Initialize cleaner\n",
        "    cleaner = TextCleaner()\n",
        "\n",
        "    print(\"\\nORIGINAL TEXT:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(sample_text)\n",
        "\n",
        "    # 1. Basic cleaning\n",
        "    print(\"\\n1. BASIC CLEANING (lowercase, punctuation, stopwords):\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        lowercase=True,\n",
        "        remove_stopwords=True,\n",
        "        remove_punctuation=True,\n",
        "        lemmatize=False\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 2. Advanced cleaning with lemmatization\n",
        "    print(\"\\n2. WITH LEMMATIZATION:\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        lowercase=True,\n",
        "        remove_stopwords=True,\n",
        "        remove_punctuation=True,\n",
        "        lemmatize=True\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 3. Removing URLs, emails, and numbers\n",
        "    print(\"\\n3. REMOVING URLs, EMAILS, AND NUMBERS:\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        remove_urls=True,\n",
        "        remove_emails=True,\n",
        "        remove_numbers=True,\n",
        "        lemmatize=True\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 4. Minimal cleaning (preserve more information)\n",
        "    print(\"\\n4. MINIMAL CLEANING (preserve case and some punctuation):\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        lowercase=False,\n",
        "        remove_stopwords=True,\n",
        "        remove_punctuation=False,\n",
        "        lemmatize=False,\n",
        "        remove_urls=True,\n",
        "        remove_emails=True\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 5. Extract named entities\n",
        "    print(\"\\n5. NAMED ENTITIES FOUND:\")\n",
        "    print(\"-\" * 40)\n",
        "    entities = cleaner.extract_entities(sample_text)\n",
        "    for text, label in entities:\n",
        "        print(f\"  - {text}: {label}\")\n",
        "\n",
        "    # 6. Remove specific POS tags\n",
        "    print(\"\\n6. REMOVE ADJECTIVES AND ADVERBS:\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.remove_by_pos(sample_text, {'ADJ', 'ADV'})\n",
        "    print(cleaned)\n",
        "\n",
        "    # 7. Custom cleaning configuration\n",
        "    print(\"\\n7. CUSTOM CONFIGURATION (strict cleaning):\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        lowercase=True,\n",
        "        remove_stopwords=True,\n",
        "        remove_punctuation=True,\n",
        "        remove_numbers=True,\n",
        "        remove_spaces=True,\n",
        "        lemmatize=True,\n",
        "        remove_emails=True,\n",
        "        remove_urls=True,\n",
        "        remove_special_chars=True,\n",
        "        min_token_length=3  # Only keep tokens with 3+ characters\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 8. Show POS tags for understanding\n",
        "    print(\"\\n8. PART-OF-SPEECH TAGS (first 20 tokens):\")\n",
        "    print(\"-\" * 40)\n",
        "    pos_tags = cleaner.get_pos_tags(sample_text)[:20]\n",
        "    for token, pos in pos_tags:\n",
        "        if not token.isspace():\n",
        "            print(f\"  {token:15} -> {pos}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CLEANING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5AX864LWXiE",
        "outputId": "717f0512-b73e-4562-8221-5f2916892faf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SPACY TEXT CLEANING PROOF OF CONCEPT\n",
            "================================================================================\n",
            "\n",
            "ORIGINAL TEXT:\n",
            "----------------------------------------\n",
            "\n",
            "    Hello World!!!  This is a SAMPLE paragraph with     various issues.\n",
            "    Visit our website at https://www.example.com or contact us at info@example.com.\n",
            "\n",
            "    The meeting is scheduled for 15th December, 2024 at 3:30 PM.\n",
            "    We'll be discussing the Q4 2024 results and planning for 2025.\n",
            "\n",
            "    Some unnecessary words: very, really, actually, basically are often overused.\n",
            "    Numbers like 123, 456.78 and special characters @#$% should be handled!\n",
            "\n",
            "    Microsoft Corporation (MSFT) announced new features.   Apple Inc. is also innovating.\n",
            "    \n",
            "\n",
            "1. BASIC CLEANING (lowercase, punctuation, stopwords):\n",
            "----------------------------------------\n",
            "hello world sample paragraph issues visit website contact meeting scheduled 15th december 2024 330 pm discussing q4 2024 results planning 2025 unnecessary words actually basically overused numbers like 123 45678 special characters handled microsoft corporation msft announced new features apple inc innovating\n",
            "\n",
            "2. WITH LEMMATIZATION:\n",
            "----------------------------------------\n",
            "hello world sample paragraph issue visit website contact meeting schedule 15th december 2024 330 pm discuss q4 2024 result planning 2025 unnecessary word actually basically overused number like 123 45678 special character handle microsoft corporation msft announce new feature apple inc innovate\n",
            "\n",
            "3. REMOVING URLs, EMAILS, AND NUMBERS:\n",
            "----------------------------------------\n",
            "hello world sample paragraph issue visit website contact meeting schedule december 330 pm discuss q4 result planning unnecessary word actually basically overused number like special character handle microsoft corporation msft announce new feature apple inc innovate\n",
            "\n",
            "4. MINIMAL CLEANING (preserve case and some punctuation):\n",
            "----------------------------------------\n",
            "Hello World SAMPLE paragraph issues Visit website contact meeting scheduled 15th December 2024 330 PM discussing Q4 2024 results planning 2025 unnecessary words actually basically overused Numbers like 123 45678 special characters handled Microsoft Corporation MSFT announced new features Apple Inc innovating\n",
            "\n",
            "5. NAMED ENTITIES FOUND:\n",
            "----------------------------------------\n",
            "  - 15th December, 2024: DATE\n",
            "  - 3:30 PM: TIME\n",
            "  - Q4 2024: DATE\n",
            "  - 2025: DATE\n",
            "  - 123: CARDINAL\n",
            "  - 456.78: DATE\n",
            "  - Microsoft Corporation: ORG\n",
            "  - Apple Inc.: ORG\n",
            "\n",
            "6. REMOVE ADJECTIVES AND ADVERBS:\n",
            "----------------------------------------\n",
            "\n",
            "     Hello World ! ! !   This is a SAMPLE paragraph with      issues . \n",
            "     Visit our website at https://www.example.com or contact us at info@example.com . \n",
            "\n",
            "     The meeting is scheduled for December , 2024 at 3:30 PM . \n",
            "     We 'll be discussing the Q4 2024 results and planning for 2025 . \n",
            "\n",
            "     Some words : , , , are . \n",
            "     Numbers like 123 , 456.78 and characters @#$% should be handled ! \n",
            "\n",
            "     Microsoft Corporation ( MSFT ) announced features .    Apple Inc. is innovating . \n",
            "    \n",
            "\n",
            "7. CUSTOM CONFIGURATION (strict cleaning):\n",
            "----------------------------------------\n",
            "hello world sample paragraph issue visit website contact meeting schedule december 330 discuss result planning unnecessary word actually basically overused number like special character handle microsoft corporation msft announce new feature apple inc innovate\n",
            "\n",
            "8. PART-OF-SPEECH TAGS (first 20 tokens):\n",
            "----------------------------------------\n",
            "  Hello           -> INTJ\n",
            "  World           -> PROPN\n",
            "  !               -> PUNCT\n",
            "  !               -> PUNCT\n",
            "  !               -> PUNCT\n",
            "  This            -> PRON\n",
            "  is              -> AUX\n",
            "  a               -> DET\n",
            "  SAMPLE          -> PROPN\n",
            "  paragraph       -> NOUN\n",
            "  with            -> ADP\n",
            "  various         -> ADJ\n",
            "  issues          -> NOUN\n",
            "  .               -> PUNCT\n",
            "  Visit           -> VERB\n",
            "  our             -> PRON\n",
            "\n",
            "================================================================================\n",
            "CLEANING COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a TextCleaner class with several methods for preprocessing text data:\n",
        "\n",
        "__init__(self, model_name=\"en_core_web_sm\"): Initializes the TextCleaner by loading a SpaCy language model. If the model is not found, it automatically downloads it.\n",
        "\n",
        "***clean_text(...):*** This is  core method for cleaning text. It takes various boolean arguments to control the cleaning process, such as converting to lowercase, removing stop words, punctuation, numbers, spaces, emails, URLs, and special characters, as well as performing lemmatization and setting a minimum token length. It uses regular expressions for initial removal of URLs and emails, and then processes the text with the loaded SpaCy model to handle other cleaning tasks based on token properties.\n",
        "\n",
        "***extract_entities(self, text: str):*** Extracts named entities (like people, organizations, dates) from the input text using the SpaCy model.\n",
        "\n",
        "***get_pos_tags(self, text: str):*** Gets the part-of-speech tag for each token in the input text.\n",
        "\n",
        "***remove_by_pos(self, text: str, pos_to_remove: Set[str]):*** Removes tokens from the text based on a provided set of part-of-speech tags.\n",
        "\n",
        "***normalize_whitespace(self, text: str)***: Cleans up various forms of whitespace in the text.\n",
        "\n",
        "The main() function provides a proof of concept by:\n",
        "\n",
        "Defining a sample_text with various cleaning challenges.\n",
        "Initializing a TextCleaner object.\n",
        "Demonstrating different text cleaning configurations using the clean_text method (basic cleaning, with lemmatization, removing specific elements, minimal cleaning, custom strict cleaning).\n",
        "Showing examples of entity extraction, removing tokens by POS tags, and displaying POS tags for understanding.\n",
        "Essentially, this script provides a flexible and powerful way to clean text data for natural language processing tasks, allowing you to customize the cleaning steps based on your specific needs."
      ],
      "metadata": {
        "id": "_b_433pQN1b6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Example 2:** Tokenize & stopword"
      ],
      "metadata": {
        "id": "5PA7VVSEXkIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "sentence = \"She sells seashells on the seashore\""
      ],
      "metadata": {
        "id": "4C2kTVqkdlbR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Then, Stopwords Removal:**\n",
        "Why? cause if you remve these you still can understand the sentence by context"
      ],
      "metadata": {
        "id": "4JXSdhSrXxJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text,stop_word_list):\n",
        "    return ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_word_list])\n",
        "\n",
        "\n",
        "custom_stop_word_list = ['she', 'on', 'the', 'am', 'is', 'not']\n",
        "remove_stop_words(sentence,custom_stop_word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JJh6oQXvdtH0",
        "outputId": "d6970b69-f100-467f-d45e-4c68c3c03cdb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sells seashells seashore'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **FEATURE EXTRACTION FROM TEXT**\n",
        "#### **General features:** statistical calculations. Do not depend on context of the text. E.G: ***Number of tokens or characters*** in text\n",
        "\n",
        "#### **Specific features**: are dependnt of the inherent meaning of text. Represent semantics of text. E.g: the ***frequency of unique words*** in the text is a specific feature.\n",
        "\n",
        "Example: 2 sentences with = # of words (4): \"The sky is blue\" & \"the pillar is yellow\".\n",
        "Same general features (# of words), but diferent  individual tokens"
      ],
      "metadata": {
        "id": "6C-4JNFzY-rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Import pandas as pd*** and ***from textblob import TextBlob***: import the  libraries, **pandas for data manipulation** and **TextBlob for text processing** (although TextBlob is not used in the selected snippet itself, it's likely used in subsequent cells).\n",
        "\n",
        "***df = pd.DataFrame(...)***: This creates a pandas DataFrame. The data is provided as a list of lists, where each inner list contains a single sentence.\n",
        "***df.columns = ['text']***: This assigns the column name 'text' to the newly created DataFrame.\n",
        "***df.head()***: This displays the first few rows of the DataFrame, which in this case is the entire DataFrame since it only has a few rows.\n",
        "\n",
        "This sets up the data to be used for further text processing and analysis demonstrated in the following cells.\n",
        "\n",
        "**EXAMPLE: Give me the # of words in this dataset**"
      ],
      "metadata": {
        "id": "LDe0Hk6BYa2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "df = pd.DataFrame([['The interim budget for 2019 will be announced on 1st February.'], ['Do you know how much expectation the middle-class working population is having from this budget?'], ['February is the shortest month in a year.'], ['This financial year will end on 31st March.']])\n",
        "df.columns = ['text']\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "5g-UsVhmehll",
        "outputId": "376454e1-075c-4040-be52-498878f9883e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  The interim budget for 2019 will be announced ...\n",
              "1  Do you know how much expectation the middle-cl...\n",
              "2          February is the shortest month in a year.\n",
              "3        This financial year will end on 31st March."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ee218b0-a1a5-441f-8bf5-553fc07e8ad7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The interim budget for 2019 will be announced ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Do you know how much expectation the middle-cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>February is the shortest month in a year.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This financial year will end on 31st March.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ee218b0-a1a5-441f-8bf5-553fc07e8ad7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ee218b0-a1a5-441f-8bf5-553fc07e8ad7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ee218b0-a1a5-441f-8bf5-553fc07e8ad7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e3d28d62-0810-4a22-b9ef-7dd2fa1d3721\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3d28d62-0810-4a22-b9ef-7dd2fa1d3721')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e3d28d62-0810-4a22-b9ef-7dd2fa1d3721 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Do you know how much expectation the middle-class working population is having from this budget?\",\n          \"This financial year will end on 31st March.\",\n          \"The interim budget for 2019 will be announced on 1st February.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### COUNT WORDS IN A SENTENCE:\n",
        "\n",
        "The code ***def add_num_words(df)*** defines and uses a function to count the number of words in each sentence in your DataFrame:\n",
        "\n",
        "***def add_num_words(df)***: This defines a fxn called add_num_words that takes a pandas DataFrame (df) as input.\n",
        "\n",
        "***df['number_of_words'] = df['text'].apply(lambda x : len(TextBlob(str(x)).words)):*** This is the core of the fxn. Creates a new column in the DataFrame called number_of_words. It applies a lambda fxn to each entry in the 'text' column (df['text'].apply(...)).\n",
        "The lambda function takes each text entry (x), converts it to a string (str(x)), creates a TextBlob object from it, accesses the .words attribute (which gives a list of words), and then calculates the length of that list (len(...)), effectively counting the words.\n",
        "\n",
        "**return df**: The function returns the modified DataFrame with the new column.\n",
        "\n",
        "***add_num_words(df)['number_of_words']***:  calls the add_num_words fxn with your DataFrame df and then selects and displays only the newly created 'number_of_words' column.\n",
        "\n",
        "###The output 0 11, 1 15, 2 8, 3 8 shows the number of words for each sentence in your DataFrame."
      ],
      "metadata": {
        "id": "ttdyuO5-bV-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_num_words(df):\n",
        "    df['number_of_words'] = df['text'].apply(lambda x : len(TextBlob(str(x)).words))\n",
        "    return df\n",
        "add_num_words(df)['number_of_words']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "Ure3qwuletnW",
        "outputId": "dffc58de-74c1-43d1-bec2-51e9562db344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    11\n",
              "1    15\n",
              "2     8\n",
              "3     8\n",
              "Name: number_of_words, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number_of_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is the intersection between some of these token and some of these wh_ words?**\n",
        "\n",
        " ***def is_present(wh_words, df):***, defines a Python fxn named is_present.\n",
        "\n",
        "  This function is designed to take two arguments: `wh_words (which is expected to be a set of words)` and df (which is expected to be a pandas DataFrame). The purpose of this function, as indicated by its name, is likely to check for the presence of words from the wh_words set within the text data contained in the DataFrame df."
      ],
      "metadata": {
        "id": "hRq81vbAaw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_present(wh_words, df):\n",
        "\n",
        "    # The below line of code will find the intersection between set of tokens of\n",
        "    #  every sentence and the wh_words and will return true if the length of intersection\n",
        "    #  set is non-zero.\n",
        "    df['is_wh_words_present'] = df['text'].apply(lambda x : True if \\\n",
        "                                                 len(set(TextBlob(str(x)).words).intersection(wh_words))>0 else False)\n",
        "    return df\n",
        "\n",
        "wh_words = set(['why', 'who', 'which', 'what', 'where', 'when', 'how'])\n",
        "\n",
        "is_present(wh_words, df)['is_wh_words_present']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "jv6eMIAee0bk",
        "outputId": "f1fcde70-3dd3-4baa-d339-1294b8a06dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    False\n",
              "1     True\n",
              "2    False\n",
              "3    False\n",
              "Name: is_wh_words_present, dtype: bool"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_wh_words_present</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> bool</label>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " #### **Import essential tools from the NLTK library** for NLP processing, specifically for splitting text into words (word_tokenize) and identifying their grammatical roles (pos_tag).\n",
        "\n",
        " #### It also **downloads the necessary data** (tagsets & averaged_perceptron_tagger) for these NLTK functions to work correctly."
      ],
      "metadata": {
        "id": "40GECdFDc5F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import nltk\n",
        "\n",
        "nltk.download('tagsets')\n",
        "from nltk.data import load\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNX_JvlofLHB",
        "outputId": "a99d19c5-ffbd-45f4-a186-56f860e5e456"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tagsets():\n",
        "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
        "    return list(tagdict.keys())\n",
        "\n",
        "tag_list = get_tagsets()\n",
        "\n",
        "print(tag_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVfIJpnufPqw",
        "outputId": "47059420-f3d8-49d8-b23a-756ea2e10873"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRP$', 'VBG', 'FW', 'VB', 'POS', \"''\", 'VBP', 'VBN', 'JJ', 'WP', 'VBZ', 'DT', 'RP', '$', 'NN', ')', '(', 'RBR', 'VBD', ',', '.', 'TO', 'LS', 'RB', ':', 'NNS', 'NNP', '``', 'WRB', 'CC', 'PDT', 'RBS', 'PRP', 'CD', 'EX', 'IN', 'WP$', 'MD', 'NNPS', '--', 'JJS', 'JJR', 'SYM', 'UH', 'WDT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BAG OF WORDS (BoW)**\n",
        "\n",
        "### **Convert every sentence into a vector.** Is the 1st algorithm to know in NLP\n",
        "\n",
        "#### The **vector's length  =  # of unique words**\n",
        "\n",
        "Every single word is assigned a unique index #: *\"The dog runs in a park\"*. Index: 0 1. 2. 3. 4. 5\n",
        "\n",
        "This is done in 2 steps:\n",
        "1. The vocabulary or dictionary of all the words is generated\n",
        "2. Every doc is represented by a list which length = # of words in that vocabulary/dictiornary.  In terms of the presence or absense of al words\n",
        "\n",
        "**Example of BoW:**\n",
        "\n",
        "* I request to give all the reviews on a restaurant for example, broken down in sentences:\n",
        "\n",
        "Review 1 : \"Great Cuban food. Amazing Cuban coffee\"\n",
        "\n",
        "Review 2: \"Terrible service. Food was terrible\"\n",
        "\n",
        "Review 3: \" Great Service. Amazing food! Great coffee\"\n",
        "\n",
        "* Then give me the unique words:\n",
        "\n",
        "**Step 1** = identify all unique words in the corpus\n",
        "\n",
        "vocabulary = ['amazing', 'coffee', 'cuban', 'food', great', 'service', 'terrible', 'was']\n",
        "\n",
        "**Step 2** = Count the words in each doc ( in this case is \"review\"):\n",
        "\n",
        "Word     Review 1.   Review 2.    Review 3.\n",
        "\n",
        "Amazing    1.      0.  1\n",
        "\n",
        "coffee  1. 0.  1\n",
        "\n",
        "Cuban 2   0. 1\n",
        "\n",
        "food  1  1. 1\n",
        "\n",
        "great 1 0 2\n",
        "\n",
        "service. 0 1  1\n",
        "\n",
        "terrible 0 2 0\n",
        "\n",
        "was 0 1 0\n",
        "\n",
        "\n",
        "**Step 3** = vectors\n",
        "review_1_vector = [1,1,2,1,1,0,0,0,0] # great cuban food...\n",
        "\n",
        "review_2_vector = [0,0,0,1,0,1,2,1]  # Terrible service...\n",
        "\n",
        "review_3_vector = [1,1,0,1,2,1,0,0,0]   # Great service..."
      ],
      "metadata": {
        "id": "BJSsSNgzdaVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================\n",
        "# Our Miami restaurant reviews\n",
        "#===================================\n",
        "\n",
        "#Import my words count vectorizar\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "reviews = [\n",
        "    \"Great Cuban food. Amazing Cuban coffee.\",\n",
        "    \"Terrible service. Food was terrible.\",\n",
        "    \"Great service. Amazing food. Great coffee.\"\n",
        "]\n",
        "\n",
        "# Create the Bag of Words matrix\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(reviews)  # I am feeding my custome data of reviews\n",
        "\n",
        "# See the vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWSqxx4KilQO",
        "outputId": "5dd0c657-0b4b-408f-9daf-fab05a3c770a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['amazing' 'coffee' 'cuban' 'food' 'great' 'service' 'terrible' 'was']\n",
            "\n",
            "Bag of Words Matrix:\n",
            "[[1 1 2 1 1 0 0 0]\n",
            " [0 0 0 1 0 1 2 1]\n",
            " [1 1 0 1 2 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **With BoW simple algorithm, we can do:**\n",
        "\n",
        " * Spam detection.\n",
        " * Categorize news articles, or content\n",
        " * Sentiment analysis (id complaints)\n",
        " * Find document similarity (plagiarism)\n",
        " * Language detection,"
      ],
      "metadata": {
        "id": "ZKEsXQtXjv_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spam detection**\n"
      ],
      "metadata": {
        "id": "NXipgwrDkjd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Miami-themed email examples\n",
        "emails = [\n",
        "    # Spam emails\n",
        "    \"WIN FREE cruise to Bahamas from Miami port! Click NOW!!!\",\n",
        "    \"URGENT! Your Miami bank account needs verification! Act fast!\",\n",
        "    \"Congratulations! You won $1000 Miami shopping spree! Claim here!\",\n",
        "    \"FREE tickets to Ultra Music Festival! Limited time offer!\",\n",
        "\n",
        "    # Normal emails\n",
        "    \"Meeting tomorrow at the Brickell office at 2pm\",\n",
        "    \"Can you send me the quarterly report when you have time?\",\n",
        "    \"Let's grab Cuban coffee after the presentation\",\n",
        "    \"The project deadline has been moved to next Friday\"\n",
        "]\n",
        "\n",
        "labels = ['spam', 'spam', 'spam', 'spam',\n",
        "          'normal', 'normal', 'normal', 'normal']\n",
        "\n",
        "# Create BoW\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "\n",
        "# Train classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X, labels)\n",
        "\n",
        "# Test new email\n",
        "new_email = [\"FREE Miami Heat tickets! Click this link now!\"]\n",
        "new_email_bow = vectorizer.transform(new_email)\n",
        "prediction = classifier.predict(new_email_bow)\n",
        "print(f\"Prediction: {prediction[0]}\")  # Output: spam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRBXzYMkjGSo",
        "outputId": "c0c718c6-7911-4ee5-8b20-dfbc52ef55f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Categorize news articles or content**: art? sports? nature?\n",
        "e.g: product categorization, or social media BoW can help you id the most popular trends or #"
      ],
      "metadata": {
        "id": "gQrzh0Ygkqta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorize Miami news articles\n",
        "articles = [\n",
        "    \"The Miami Heat defeated the Lakers in overtime last night at the arena\",\n",
        "    \"New coral restoration project launches in Biscayne Bay this week\",\n",
        "    \"Art Basel Miami Beach announces featured artists for 2024 exhibition\",\n",
        "    \"Hurricane season preparation tips for South Florida residents\",\n",
        "    \"Dolphins quarterback throws three touchdowns in victory\",\n",
        "    \"Climate change impacts on Miami Beach erosion studied by scientists\"\n",
        "]\n",
        "\n",
        "categories = ['sports', 'environment', 'arts', 'weather', 'sports', 'environment']\n",
        "\n",
        "# BoW + Classification\n",
        "vectorizer = CountVectorizer(max_features=50)\n",
        "X = vectorizer.fit_transform(articles)\n",
        "# Now you can train any classifier (SVM, Random Forest, etc.)"
      ],
      "metadata": {
        "id": "pm7M9F1AjM7_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Sentiment analysis**\n",
        "Positive or Negative review?"
      ],
      "metadata": {
        "id": "40GlVNjqk0hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Miami restaurant reviews\n",
        "reviews = [\n",
        "    \"Amazing Cuban sandwich! Best in Miami! Will definitely return!\",\n",
        "    \"Terrible service, cold food, never coming back to this place\",\n",
        "    \"Decent food but nothing special, average Miami restaurant\",\n",
        "    \"Horrible experience, worst meal ever, completely disappointed\",\n",
        "    \"Outstanding seafood! Fresh catch! Excellent service! Love it!\",\n",
        "    \"Mediocre at best, expected more from the reviews\"\n",
        "]\n",
        "\n",
        "sentiments = ['positive', 'negative', 'neutral', 'negative', 'positive', 'neutral']\n",
        "\n",
        "# Create BoW\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(reviews)\n",
        "\n",
        "# Train sentiment classifier\n",
        "sentiment_classifier = LogisticRegression()\n",
        "sentiment_classifier.fit(X, sentiments)\n",
        "\n",
        "# Analyze new review\n",
        "new_review = [\"The food was fantastic and service was great!\"]\n",
        "new_bow = vectorizer.transform(new_review)\n",
        "sentiment = sentiment_classifier.predict(new_bow)\n",
        "print(f\"Sentiment: {sentiment[0]}\")  # Output: positive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skmImh8qjRkt",
        "outputId": "7a261087-0a02-4a37-d5d4-4fae19982d75"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Find document similarity** (plagiarism or similar reviews)"
      ],
      "metadata": {
        "id": "knweIfQek7TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Miami event descriptions\n",
        "events = [\n",
        "    \"Beach volleyball tournament at South Beach this Saturday\",\n",
        "    \"Sand volleyball competition on South Beach this weekend\",\n",
        "    \"Art gallery opening in Wynwood Arts District Friday night\",\n",
        "    \"New exhibition opens at Wynwood Walls this Friday evening\",\n",
        "    \"Food truck festival at Bayfront Park all weekend\"\n",
        "]\n",
        "\n",
        "# Convert to BoW\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(events)\n",
        "\n",
        "# Calculate similarity\n",
        "similarity_matrix = cosine_similarity(bow_matrix)\n",
        "\n",
        "# Find most similar events to event 0\n",
        "event_idx = 0\n",
        "similarities = similarity_matrix[event_idx]\n",
        "most_similar_idx = similarities.argsort()[-2]  # -1 would be itself\n",
        "\n",
        "print(f\"Event: {events[event_idx]}\")\n",
        "print(f\"Most similar: {events[most_similar_idx]}\")\n",
        "print(f\"Similarity score: {similarities[most_similar_idx]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqebCkcCjU1h",
        "outputId": "ba836ead-6a50-4f40-8f38-acdd1bf17a91"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event: Beach volleyball tournament at South Beach this Saturday\n",
            "Most similar: Sand volleyball competition on South Beach this weekend\n",
            "Similarity score: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This code implements a simple search engine using the **Bag of Words model and cosine similarity** to **find documents most similar to a given query.**"
      ],
      "metadata": {
        "id": "klaYAu7il0dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_documents(query, documents):\n",
        "    \"\"\"Simple BoW-based search engine\"\"\"\n",
        "\n",
        "    # Combine query with documents\n",
        "    all_text = [query] + documents\n",
        "\n",
        "    # Create BoW\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_matrix = vectorizer.fit_transform(all_text)\n",
        "\n",
        "    # Calculate similarity between query (index 0) and all docs\n",
        "    query_vector = bow_matrix[0]\n",
        "    doc_vectors = bow_matrix[1:]\n",
        "\n",
        "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
        "\n",
        "    # Rank documents\n",
        "    ranked_idx = similarities.argsort()[::-1]\n",
        "\n",
        "    return [(documents[idx], similarities[idx]) for idx in ranked_idx if similarities[idx] > 0]\n",
        "\n",
        "# Miami business descriptions\n",
        "businesses = [\n",
        "    \"Joe's Stone Crab serves fresh seafood and famous stone crabs\",\n",
        "    \"Versailles Restaurant offers authentic Cuban cuisine and coffee\",\n",
        "    \"Books & Books is an independent bookstore with author events\",\n",
        "    \"Jungle Island features exotic animals and interactive shows\",\n",
        "    \"Pérez Art Museum Miami showcases contemporary and modern art\"\n",
        "]\n",
        "\n",
        "# Search\n",
        "results = search_documents(\"Cuban food restaurant\", businesses)\n",
        "for doc, score in results[:3]:\n",
        "    print(f\"Score: {score:.2f} - {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT81ww9GjiCe",
        "outputId": "ea8e9fcd-e2aa-47ad-8d87-83190490c8a0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.41 - Versailles Restaurant offers authentic Cuban cuisine and coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***def search_documents(query, documents):*** function search_documents that takes a search query string and a list of documents (strings) as input.\n",
        "\n",
        "***all_text = [query] + documents***: Combines the query and all documents into a single list of strings.\n",
        "\n",
        "***vectorizer = CountVectorizer(): ***Creates an instance of CountVectorizer to convert text into a Bag of Words representation.\n",
        "\n",
        "***bow_matrix = vectorizer.fit_transform(all_text)***: Fits the vectorizer to all the text (query + documents) and transforms them into a BoW matrix.\n",
        "\n",
        "***query_vector = bow_matrix[0]***: Selects the BoW vector for the query (which is the first item in all_text).\n",
        "\n",
        "***doc_vectors = bow_matrix[1:]***: Selects the BoW vectors for all the documents.\n",
        "\n",
        "***similarities = cosine_similarity(query_vector, doc_vectors).flatten():*** Calculates the cosine similarity between the query vector and each document vector. flatten() converts the result into a 1D array.\n",
        "\n",
        "***ranked_idx = similarities.argsort()[::-1]:*** Gets the indices that would sort the similarities array in descending order.\n",
        "\n",
        "***return [(documents[idx], similarities[idx]) for idx in ranked_idx if similarities[idx] > 0]:*** Returns a list of tuples, where each tuple contains a document and its similarity score, sorted from highest similarity to lowest, and only includes documents with a similarity score greater than 0.\n",
        "\n",
        "The code then defines a list of sample Miami business descriptions, calls the search_documents function with a query, and prints the top 3 results."
      ],
      "metadata": {
        "id": "z0uE-V8VmKus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "#=======================\n",
        "# Miami news snippets\n",
        "#=======================\n",
        "\n",
        "news_snippets = [\n",
        "    \"Hurricane preparedness workshop scheduled for residents\",\n",
        "    \"Beach erosion concerns grow after recent storms\",\n",
        "    \"Heat win playoff game with Butler scoring 35 points\",\n",
        "    \"Dolphins draft new quarterback in first round\",\n",
        "    \"Art Basel brings international artists to Miami Beach\",\n",
        "    \"Storm surge warnings issued for coastal areas\",\n",
        "    \"Museum opens new contemporary art exhibition\",\n",
        "    \"Basketball team advances to conference finals\"\n",
        "]\n",
        "\n",
        "# Create BoW\n",
        "vectorizer = CountVectorizer(max_features=20, stop_words='english')\n",
        "bow = vectorizer.fit_transform(news_snippets)\n",
        "\n",
        "# Topic modeling\n",
        "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
        "lda.fit(bow)\n",
        "\n",
        "# Display topics\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words_idx = topic.argsort()[-5:][::-1]\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxtIg0y_jn9V",
        "outputId": "6e115aaa-4388-4459-aa74-1065096bb952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: beach, art, artists, brings, basel\n",
            "Topic 2: new, exhibition, contemporary, draft, dolphins\n",
            "Topic 3: finals, advances, basketball, conference, areas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#===================\n",
        "#HOTEL FEEDBACK\n",
        "#===================\n",
        "feedback = [\n",
        "    \"Great location near beach but parking was expensive\",\n",
        "    \"Beautiful beach view and excellent pool area\",\n",
        "    \"Parking was terrible and very expensive\",\n",
        "    \"Love the beach access and pool facilities\",\n",
        "    \"Room was clean but parking situation is horrible\"\n",
        "]\n",
        "\n",
        "# Create BoW focusing on specific aspects\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Include bigrams\n",
        "bow = vectorizer.fit_transform(feedback)\n",
        "\n",
        "# Sum word frequencies\n",
        "word_freq = bow.sum(axis=0).A1\n",
        "word_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Find most mentioned aspects\n",
        "import numpy as np\n",
        "top_indices = word_freq.argsort()[-10:][::-1]\n",
        "\n",
        "print(\"Most mentioned aspects:\")\n",
        "for idx in top_indices:\n",
        "    print(f\"  '{word_names[idx]}': {word_freq[idx]} mentions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY2bVg_1jqX4",
        "outputId": "b1cd2f61-afa5-4222-ac26-3b67df2fc21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most mentioned aspects:\n",
            "  'was': 3 mentions\n",
            "  'parking': 3 mentions\n",
            "  'and': 3 mentions\n",
            "  'beach': 3 mentions\n",
            "  'pool': 2 mentions\n",
            "  'parking was': 2 mentions\n",
            "  'but parking': 2 mentions\n",
            "  'expensive': 2 mentions\n",
            "  'but': 2 mentions\n",
            "  'was clean': 1 mentions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================\n",
        "# Miami's multilingual environment\n",
        "#===================================\n",
        "\n",
        "texts = [\n",
        "    \"Welcome to Miami Beach, enjoy your stay!\",  # English\n",
        "    \"Bienvenido a Miami Beach, disfruta tu estancia!\",  # Spanish\n",
        "    \"The best Cuban coffee in all of Miami\",  # English\n",
        "    \"El mejor café cubano de todo Miami\",  # Spanish\n",
        "    \"Bienvenue à Miami Beach!\"  # French\n",
        "]\n",
        "\n",
        "languages = ['english', 'spanish', 'english', 'spanish', 'french']\n",
        "\n",
        "# Character-level BoW can help detect languages\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))\n",
        "X = char_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Train language detector\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "lang_detector = MultinomialNB()\n",
        "lang_detector.fit(X, languages)\n",
        "\n",
        "# Detect language\n",
        "new_text = [\"Hola, dónde está la playa?\"]\n",
        "new_bow = char_vectorizer.transform(new_text)\n",
        "detected = lang_detector.predict(new_bow)\n",
        "print(f\"Detected language: {detected[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg-gifcIjvkl",
        "outputId": "93415af3-6072-49ed-dfc3-e70c79e12fdb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: spanish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Activity 2.01: Extracting Top Keywords from the News Article\n",
        "\n",
        "In this activity, you will extract the most frequently occurring keywords from a sample news article using Python and the Natural Language Toolkit (NLTK).\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Basic understanding of Python programming.\n",
        "- An environment to run Python code (like Jupyter Notebook or Google Colab).\n",
        "\n",
        "### Data\n",
        "\n",
        "The news article used in this activity is available at the following link: [news_article.txt](https://github.com/fenago/natural-language-processing-workshop/blob/master/Lab02/data/news_article.txt).\n",
        "\n",
        "### Steps to Follow\n",
        "\n",
        "1. **Set Up Your Environment**:\n",
        "   - Open Jupyter Notebook or Google Colab.\n",
        "   - Ensure Python is installed along with NLTK. Install NLTK if not already installed using `!pip install nltk`.\n",
        "\n",
        "2. **Import Necessary Libraries**:\n",
        "   - Import `nltk` and other necessary Python libraries.\n",
        "\n",
        "3. **Define Helper Functions**:\n",
        "   - Create functions to load the text file, convert text to lowercase, tokenize the text, remove stop words, perform stemming, and calculate word frequencies.\n",
        "\n",
        "4. **Load the News Article**:\n",
        "   - Use Python's file handling methods to load `news_article.txt` into a string.\n",
        "\n",
        "5. **Preprocess the Text**:\n",
        "   - Convert the text to lowercase.\n",
        "   - Tokenize the text using a whitespace tokenizer.\n",
        "   - Remove stop words from the tokens.\n",
        "   - Perform stemming on the remaining tokens.\n",
        "\n",
        "6. **Calculate Word Frequencies**:\n",
        "   - Count the frequency of each word after stemming.\n",
        "   - Display the most frequent keywords.\n",
        "\n",
        "### Challenge for Students\n",
        "\n",
        "Now that you've learned how to extract keywords from a news article, challenge yourself by applying these techniques to a different dataset. Here's what you can do:\n",
        "\n",
        "- **Find a Unique Dataset**: Select a text dataset of your interest. This could be another news article, a blog post, or any textual data.\n",
        "- **Implement the Keyword Extraction Process**: Apply the steps you've learned in this activity to your dataset. This includes text preprocessing, tokenization, stop word removal, stemming, and frequency analysis.\n",
        "- **Analyze Your Results**: Look at the most frequent keywords in your dataset. Do they give you insights into the main themes or topics of the text?\n",
        "\n",
        "**Contextualize Your Learning**: Reflect on how this process could be useful in real-world applications like search engine optimization, content analysis, or summarizing information.\n"
      ],
      "metadata": {
        "id": "A3UA0n3bkRCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for the news_article.txt - just create a text file and put this into the contents:\n",
        "\n",
        "Ever since the populist Law and Justice (pis) party took power in 2015, Adam Bodnar, Poland’s\n",
        " human-rights ombudsman, has been warning against its relentless efforts to get control of the\n",
        " courts. To illustrate the danger, he uses an expression from communist times: lex telefonica.\n",
        " In the Polish People’s Republic, verdicts were routinely dictated by a phone call from an\n",
        " apparatchik at party headquarters. Today’s government has more subtle techniques,\n",
        " but the goal is the same, Mr Bodnar says: “If a judge has a case on his desk with some\n",
        " political importance, he should be afraid.”\n",
        "\n",
        "The European Commission is worried, too. It accuses pis of violating Poland’s commitments\n",
        "to the rule of law under the European Union’s founding treaty. In 2017 the commission took\n",
        "Poland to the European Court of Justice (ecj) over laws that gave politicians control over\n",
        "appointing judges. (For example, they lowered judges’ retirement age while letting the justice\n",
        " minister pick whom to exempt.) The ecj ruled against the Poles, who had in the meantime\n",
        " scrapped some of the measures."
      ],
      "metadata": {
        "id": "Q89dgrN3ku31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activity 2.02: Text Visualization\n",
        "\n",
        "In this activity, you will create a word cloud for the 50 most frequent words in a dataset. The dataset consists of random sentences that need to be cleaned and analyzed to identify frequently occurring words.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Basic understanding of Python programming.\n",
        "- Familiarity with text processing and visualization libraries in Python.\n",
        "\n",
        "### Data\n",
        "\n",
        "The dataset used in this activity is available at the following link: [text_corpus.txt](https://github.com/fenago/natural-language-processing-workshop/blob/master/Lab02/data/text_corpus.txt\n",
        ").\n",
        "\n",
        "### Steps to Follow\n",
        "\n",
        "1. **Import Necessary Libraries**:\n",
        "   - Import libraries required for data fetching, text processing, and visualization (like `pandas`, `nltk`, `matplotlib`, `wordcloud`, etc.).\n",
        "\n",
        "2. **Fetch the Dataset**:\n",
        "   - Retrieve the `text_corpus.txt` file and load its contents.\n",
        "\n",
        "3. **Preprocess the Text**:\n",
        "   - Perform text cleaning to remove unwanted characters and formats.\n",
        "   - Tokenize the text.\n",
        "   - Apply lemmatization to convert words to their base form.\n",
        "\n",
        "4. **Identify Top 50 Words**:\n",
        "   - Calculate the frequency of each word in the cleaned dataset.\n",
        "   - Create a set of the top 50 most frequent words along with their frequencies.\n",
        "\n",
        "5. **Create a Word Cloud**:\n",
        "   - Use the word cloud library to visualize the top 50 words.\n",
        "   - Customize the word cloud's appearance as needed.\n",
        "\n",
        "6. **Analyze the Word Cloud**:\n",
        "   - Compare the word cloud with the calculated word frequencies.\n",
        "   - Justify the representation of words in the word cloud based on their frequencies.\n",
        "\n",
        "### Challenge for Students\n",
        "\n",
        "Now that you have created a word cloud for a given dataset, try extending your skills with these tasks:\n",
        "\n",
        "- **Use a Different Dataset**: Find another text dataset that interests you. It could be a collection of social media posts, reviews, or any other textual content.\n",
        "- **Apply Enhanced Text Processing**: Experiment with different preprocessing techniques like stop word removal, n-grams, or POS tagging.\n",
        "- **Visualize Your Findings**: Create a word cloud for your chosen dataset. How does the word cloud reflect the key themes or sentiments in the data?\n",
        "- **Draw Insights**: Reflect on how word clouds can aid in quick data analysis, highlighting key areas for deeper exploration.\n",
        "\n",
        "**Explore Further**: Consider how word clouds can be used in areas like marketing analysis, sentiment analysis, or summarizing large volumes of text.\n"
      ],
      "metadata": {
        "id": "I9wtWaxaljXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_article = '''Ever since the populist Law and Justice (pis) party took power in 2015, Adam Bodnar, Poland’s human-rights ombudsman, has been warning against its relentless efforts to get control of the courts. To illustrate the danger, he uses an expression from communist times: lex telefonica. In the Polish People’s Republic, verdicts were routinely dictated by a phone call from an apparatchik at party headquarters. Today’s government has more subtle techniques, but the goal is the same, Mr Bodnar says: “If a judge has a case on his desk with some political importance, he should be afraid.”\n",
        "\n",
        "The European Commission is worried, too. It accuses pis of violating Poland’s commitments to the rule of law under the European Union’s founding treaty. In 2017 the commission took Poland to the European Court of Justice (ecj) over laws that gave politicians control over appointing judges. (For example, they lowered judges’ retirement age while letting the justice minister pick whom to exempt.) The ecj ruled against the Poles, who had in the meantime scrapped some of the measures.'''\n"
      ],
      "metadata": {
        "id": "smnw-hnrlbTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2u35-_fFlfHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_article"
      ],
      "metadata": {
        "id": "RlfC1WGEni4m",
        "outputId": "b4d158c9-b7db-42b2-be49-2182ac67f723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ever since the populist Law and Justice (pis) party took power in 2015, Adam Bodnar, Poland’s human-rights ombudsman, has been warning against its relentless efforts to get control of the courts. To illustrate the danger, he uses an expression from communist times: lex telefonica. In the Polish People’s Republic, verdicts were routinely dictated by a phone call from an apparatchik at party headquarters. Today’s government has more subtle techniques, but the goal is the same, Mr Bodnar says: “If a judge has a case on his desk with some political importance, he should be afraid.”\\n\\nThe European Commission is worried, too. It accuses pis of violating Poland’s commitments to the rule of law under the European Union’s founding treaty. In 2017 the commission took Poland to the European Court of Justice (ecj) over laws that gave politicians control over appointing judges. (For example, they lowered judges’ retirement age while letting the justice minister pick whom to exempt.) The ecj ruled against the Poles, who had in the meantime scrapped some of the measures.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GRrsmTvfnksN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GigiQR99/NLP-exercise/blob/main/NLP_RQA_wk01_Stem_Lemmatize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CAI2300C - Introduction to NLP**\n",
        "\n",
        "## **Raquel Quintanilla**  |  9/5/2025\n",
        "\n",
        "\n",
        "## **NLTK (Natural Languages Tool Kit)** = Use it to **clean data for NLP**\n",
        "### It is an platform for building Python programs (https://www.nltk.org/) to work with human language data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vcg7l9wLs3Jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLTK libraries import**"
      ],
      "metadata": {
        "id": "MbOoNBAV99Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n"
      ],
      "metadata": {
        "id": "LWidsootVEu_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Descargar varios paquetes de datos necesarios para trabajar con la biblioteca NLTK (Natural Language Toolkit) en Python.**\n",
        "\n",
        "**punkt:** Este paquete contiene modelos pre-entrenados para la tokenización de texto, que es el proceso de dividir el texto en unidades más pequeñas como palabras o frases. Es útil para sent_tokenize y word_tokenize.\n",
        "\n",
        "**stopwords:**  contiene una lista de palabras comunes (\"the\", \"a\", \"is\") que a menudo se eliminan en el preprocesamiento de texto porque no suelen aportar mucho significado.\n",
        "\n",
        "**wordnet:** Este es una base de datos léxica para el idioma inglés que agrupa palabras en conjuntos de sinónimos (synsets) y define relaciones entre ellos. ***Es fundamental para la lematización.***\n",
        "\n",
        "**averaged_perceptron_tagger:** Contiene un modelo para el etiquetado de partes del discurso (POS tagging), que asigna una etiqueta gramatical a cada palabra en una oración (sustantivo, verbo, adjetivo, etc.).\n",
        "\n",
        "**maxent_ne_chunker:** Contiene un modelo para la identificación de entidades nombradas (NER), que detecta y clasifica nombres propios (personas, organizaciones, ubicaciones, etc.) en el texto.\n",
        "\n",
        "**punkt_tab:** Datos complementarios para el tokenizador Punkt.\n",
        "averaged_perceptron_tagger_eng: Datos específicos para el etiquetador POS en inglés.\n",
        "\n",
        "**maxent_ne_chunker_tab:** Datos complementarios para el identificador de entidades nombradas.\n",
        "\n",
        "**words:**  lista de palabras en inglés, para tareas de procesamiento de texto."
      ],
      "metadata": {
        "id": "opLXilXwuRl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Download required NLTK data\n",
        "# =============================\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLz529dRbfrk",
        "outputId": "03fc87f6-9820-4e5f-e445-07412aba9f89"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PART 1: STEMMING - The Chopping Approach\n",
        "# ============================================\n",
        "\n",
        "# Data\n",
        "\n",
        "# GET SOME RAMDOM TEXT DATA HERE  THAT WE CAN USE TO TOKENIZED, TO SPLIT IN CHUNKS\n",
        "# create a big long string\n",
        "data = '''The concept of ikigai is said to have evolved from the basic health and wellness principles of traditional Japanese medicine.\n",
        " This medical tradition holds that physical wellbeing is affected by one’s mental and emotional health and sense of purpose in life.\n",
        "Japanese psychologist Michiko Kumano (2017) has said that ikigai is a state of wellbeing that arises from devotion to activities one enjoys, which also brings a sense of fulfillment.'''"
      ],
      "metadata": {
        "id": "rD7Y9KfRb2FI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **TOKENIZE PARAGRAPH: SPLIT INTO SENTENCES**\n",
        "\n",
        "### ***NLTK.SENT_TOKNIZE (DATA):***\n",
        " Tokenize by sentence, 3 sentences in this case & separates each sentence with a [,]. This Fxn **takes a string (data in this case) & splits it into a list of sentences** uses an **unsupervised algorithm** to determine sentence boundaries,***VERY useful for handling variations in punctuation and sentence structures.***"
      ],
      "metadata": {
        "id": "OLs6m2gTgMrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.sent_tokenize(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQUwMGrvcsgr",
        "outputId": "baf2424b-657b-436b-e43b-2c3c3db7617e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The concept of ikigai is said to have evolved from the basic health and wellness principles of traditional Japanese medicine.',\n",
              " 'This medical tradition holds that physical wellbeing is affected by one’s mental and emotional health and sense of purpose in life.',\n",
              " 'Japanese psychologist Michiko Kumano (2017) has said that ikigai is a state of wellbeing that arises from devotion to activities one enjoys, which also brings a sense of fulfillment.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**TOKENIZE THE PARAGRAPH: SPLIT INTO WORDS**\n",
        "### ***\"nltk.word_tokenize(data)\"***\n",
        "Function that takes a string (data in this case) and **splits it into a list of  individual words & punctuation marks**. It's a fundamental step in many NLP tasks as it breaks down raw text into smaller, manageable units.\n",
        "\n",
        "### ***Platform.opeanai.com/tokenizer***\n",
        " Split the text in tokens by stems\n"
      ],
      "metadata": {
        "id": "NLZ5qdHXgEPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.word_tokenize(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lj_n8kzcgDSz",
        "outputId": "6419801b-b326-4bb1-f35c-02cafd27d7ce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'concept',\n",
              " 'of',\n",
              " 'ikigai',\n",
              " 'is',\n",
              " 'said',\n",
              " 'to',\n",
              " 'have',\n",
              " 'evolved',\n",
              " 'from',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'health',\n",
              " 'and',\n",
              " 'wellness',\n",
              " 'principles',\n",
              " 'of',\n",
              " 'traditional',\n",
              " 'Japanese',\n",
              " 'medicine',\n",
              " '.',\n",
              " 'This',\n",
              " 'medical',\n",
              " 'tradition',\n",
              " 'holds',\n",
              " 'that',\n",
              " 'physical',\n",
              " 'wellbeing',\n",
              " 'is',\n",
              " 'affected',\n",
              " 'by',\n",
              " 'one',\n",
              " '’',\n",
              " 's',\n",
              " 'mental',\n",
              " 'and',\n",
              " 'emotional',\n",
              " 'health',\n",
              " 'and',\n",
              " 'sense',\n",
              " 'of',\n",
              " 'purpose',\n",
              " 'in',\n",
              " 'life',\n",
              " '.',\n",
              " 'Japanese',\n",
              " 'psychologist',\n",
              " 'Michiko',\n",
              " 'Kumano',\n",
              " '(',\n",
              " '2017',\n",
              " ')',\n",
              " 'has',\n",
              " 'said',\n",
              " 'that',\n",
              " 'ikigai',\n",
              " 'is',\n",
              " 'a',\n",
              " 'state',\n",
              " 'of',\n",
              " 'wellbeing',\n",
              " 'that',\n",
              " 'arises',\n",
              " 'from',\n",
              " 'devotion',\n",
              " 'to',\n",
              " 'activities',\n",
              " 'one',\n",
              " 'enjoys',\n",
              " ',',\n",
              " 'which',\n",
              " 'also',\n",
              " 'brings',\n",
              " 'a',\n",
              " 'sense',\n",
              " 'of',\n",
              " 'fulfillment',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **STEMMING ALGORITHMS**\n",
        "### Reduce words to their ***root***, by **removing sufix or prefixes**. E.g: 'Run' (root), 'Runner', 'running' are related words = 'run'\n",
        "### **GOAL**: group related words together\n",
        "Useful for **text analysis, info retrieval**,etc\n",
        "    \n",
        "\n",
        "1.   **Porter Stemming algorhythm**: Truncate the word to its root, reducing vector space, is less agressive that other stemmer.\n",
        "2.   **Lancaster Stemming algorhythm**: Iterative algo 120 rules, that loops until no more rules can be applied. E.g: \"University\" will be reduce to Universal, then to universe, till cannot reduce more.\n",
        "3. **Snowball Stemmer**: also created by Martin Porter. Improve version of Porter\n",
        "algo, more systematic and consistent.\n",
        "4. **Stopwords stemmer**: words that not add any value to the meaning. E.g\n",
        "articles: \"the\" \"an\" \"a\", etc\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nMIFEQKTi_cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer, SnowballStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "BIzBwf0JjyqM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lancaster = LancasterStemmer()\n",
        "porter = PorterStemmer()\n",
        "Snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "# PorterStemmer() - in NLTK is one of the most commonly used stemming algorithms. It's less aggressive than Lancaster and is designed\n",
        "# to strike a balance between reducing words to a common base and maintaining meaning.\n",
        "print('Porter stemmer')\n",
        "print(porter.stem(\"hobby\"))\n",
        "print(porter.stem(\"hobbies\"))\n",
        "print(porter.stem(\"computer\"))\n",
        "print(porter.stem(\"computation\"))\n",
        "print(\"**************************\")\n",
        "\n",
        "# LancasterStemmer() in NLTK is a more aggressive stemming algorithm compared to others like Porter or Snowball.\n",
        "print('lancaster stemmer')\n",
        "print(lancaster.stem(\"hobby\"))\n",
        "print(lancaster.stem(\"hobbies\"))\n",
        "print(lancaster.stem(\"computer\"))\n",
        "print(porter.stem(\"computation\"))\n",
        "print(\"**************************\")\n",
        "\n",
        "# SnowballStemmer() - It’s more consistent and linguistically refined — and it supports multiple languages (unlike Porter and Lancaster, which are English-only).\n",
        "print('Snowball stemmer')\n",
        "print(Snowball.stem(\"hobby\"))\n",
        "print(Snowball.stem(\"hobbies\"))\n",
        "print(Snowball.stem(\"computer\"))\n",
        "print(Snowball.stem(\"computation\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVvgy26Olgkl",
        "outputId": "b1175abc-a1d8-4250-fea1-321a82ba8723",
        "collapsed": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stemmer\n",
            "hobbi\n",
            "hobbi\n",
            "comput\n",
            "comput\n",
            "**************************\n",
            "lancaster stemmer\n",
            "hobby\n",
            "hobby\n",
            "comput\n",
            "comput\n",
            "**************************\n",
            "Snowball stemmer\n",
            "hobbi\n",
            "hobbi\n",
            "comput\n",
            "comput\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming example with For Loops**\n",
        "\n",
        "###This code snippet demonstrates stemming using different NLTK stemmers (Snowball, Lancaster, and Porter) on a sample sentence.\n",
        "\n",
        "***sent = \"I was going to the office on my bike when\\ I saw a car passing by hit the tree\"***: This line defines a string variable sent containing the sample sentence.\n",
        "\n",
        "***token = list(nltk.word_tokenize(sent)):*** This line tokenizes the sentence into a list of individual words and punctuation marks using nltk.word_tokenize().\n",
        "\n",
        "***for stemmer in (snowball, lancaster, porter)***:: This loop iterates through the three stemmer objects that were initialized in a previous cell.\n",
        "\n",
        "***stemm = [stemmer.stem(t) for t in token]***: This is a list comprehension that applies the stem() method of the current stemmer to each token in the token list. This produces a new list stemm containing the stemmed words.\n",
        "\n",
        "***print(\" \".join(stemm)):*** This line joins the stemmed words in the stemm list back into a single string with spaces in between and prints the resulting string.\n",
        "\n",
        "*The output shows how each stemmer reduces the words in the sentence to their root form, highlighting the differences in their aggressiveness.*"
      ],
      "metadata": {
        "id": "fP-LMHmcmYe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"I was going to my house on my car when \\\n",
        "i saw a car hitting that tree.\"\n",
        "\n",
        "token = list(nltk.word_tokenize(sent))\n",
        "for stemmer in (Snowball, lancaster, porter):\n",
        "    stemm = [stemmer.stem(t) for t in token]\n",
        "    print(\" \".join(stemm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fozutD3z29f",
        "outputId": "c259d7ff-72be-4fb1-d518-8f723bc792f2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i was go to my hous on my car when i saw a car hit that tree .\n",
            "i was going to my hous on my car when i saw a car hit that tre .\n",
            "i wa go to my hous on my car when i saw a car hit that tree .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming Example:**\n",
        "## **Goal:** Reduce de algorithm\n",
        "### 1) Sentence tokenizer\n",
        "### 2) Stemmer passed inside sentences"
      ],
      "metadata": {
        "id": "OxSnYGilp8WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph = '''While social media can be a great way to reconnect with long-lost friends and share personal news and milestones, it can also be a source of negativity and information overload.\n",
        " If you find yourself getting angry, depressed, or stressed while scrolling, set some social media limits.\n",
        " Consider deleting apps from your phone so you only log in when you’re at your computer, or even temporarily deactivating your accounts.\n",
        " And if you feel calmer with less screen time, consider signing off social media entirely.'''"
      ],
      "metadata": {
        "id": "3o1ieGE41FAR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "#Stemming\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFQVOWMRoxEY",
        "outputId": "02229313-e898-487e-dcfc-beb683a57153"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['while social media great way reconnect long-lost friend share person news mileston , also sourc neg inform overload .',\n",
              " 'if find get angri , depress , stress scroll , set social media limit .',\n",
              " 'consid delet app phone log ’ comput , even temporarili deactiv account .',\n",
              " 'and feel calmer less screen time , consid sign social media entir .']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ❌ **Example of Stemmer getting it wrong**\n",
        "\n",
        "### Stemming is not necessarily good for accuracy"
      ],
      "metadata": {
        "id": "7qoBULrbro5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(stemmer.stem('university'))\n",
        "print(stemmer.stem('universe'))\n",
        "print(stemmer.stem('universal'))\n",
        "\n",
        "# OUTPUT = 'univers'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN2mkFXWrco0",
        "outputId": "9b0acf9e-dca8-4612-9df0-844736443100"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "univers\n",
            "univers\n",
            "univers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('organization')\n",
        "stemmer.stem('organ')\n",
        "stemmer.stem('organic')\n",
        "\n",
        "# OUPUT will be = 'organ'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-RUx6A1bp37W",
        "outputId": "5815ba0d-426a-4bda-8355-8fbdc326a2f9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'organ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Is there a better way to stemming this word w/o messing their meaning?**\n",
        "### Yes! **Lemmatizer** from Google ---> this is a open source from Google\n",
        "### Is like an smart diccionary ✅\n",
        "\n",
        "### ***from nltk.stem import WordNetLemmatizer***\n",
        "\n",
        "### Example: It is going to ***understand context***, for example can differenciate \"army\" from \"arm\"(limb) by\n"
      ],
      "metadata": {
        "id": "JXOHPAlLszRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "metadata": {
        "id": "qDlMEVy8tnEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# STEMMING (Blind Chopping)\n",
        "print(stemmer.stem(\"better\"))      # → \"better\"\n",
        "print(stemmer.stem(\"good\"))       # → \"good\"\n",
        "\n",
        "# LEMMATIZATION (UNDERSTANDS THE RELATIONSHIP!)\n",
        "print(lemmatizer.lemmatize(\"better\", pos='a'))  # → \"good\"\n",
        "print(lemmatizer.lemmatize(\"good\", pos='a'))    # → \"good\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSb9oJZMt4D6",
        "outputId": "8354739c-2bf9-41cc-a64e-2c95c9bf4ab6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "better\n",
            "good\n",
            "good\n",
            "good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ **LEMMATIZER also knows grammar**\n",
        "### Is **Intelligent root Stemming**\n",
        "### It preserve the semantic meaning\n",
        "### You tokenize a corpus of data preserving its semmantics"
      ],
      "metadata": {
        "id": "NsYF6L0Ju08-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The word \"saw\" is it a tool or past tense of 'see'?\n",
        "\n",
        "# As a verb (past tense of 'see')\n",
        "print(lemmatizer.lemmatize(\"saw\", pos='v'))  # → \"see\" ('v' verb)\n",
        "\n",
        "# As a noun (cutting tool)\n",
        "print(lemmatizer.lemmatize(\"saw\", pos='n'))  # → \"saw\" ('n' noun)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL583T5wu-9d",
        "outputId": "b136b5e5-01b5-4c2d-e09b-b29664f6bdbe"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saw\n",
            "saw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('running'))\n",
        "print(lemma.lemmatize('runs'))\n",
        "print(lemma.lemmatize('ran'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C_jssZqwRqu",
        "outputId": "74d24b82-e580-4fa9-f389-4b3d43c97c82"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\n",
            "run\n",
            "ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# LEMMATIZATON\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUvKDlSDwkRL",
        "outputId": "14fee749-d290-492d-f8b5-075c35200e05"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['While social medium great way reconnect long-lost friend share personal news milestone , also source negativity information overload .',\n",
              " 'If find getting angry , depressed , stressed scrolling , set social medium limit .',\n",
              " 'Consider deleting apps phone log ’ computer , even temporarily deactivating account .',\n",
              " 'And feel calmer less screen time , consider signing social medium entirely .']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **WHAT ARE THE STOP WORDS IN ENGLISH?** ✋\n",
        "\n",
        "### This words do not add any algorithm value\n",
        "\n",
        "###**Why you do this?**\n",
        "\n",
        "\n",
        "1.   You Tokenize your data\n",
        "2.   Lemmatize it\n",
        "3. Remove the stop words from your data\n",
        "4. Clean data ready to pass for algorythm\n",
        "\n"
      ],
      "metadata": {
        "id": "NawR6PS3xbg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import stopwords alorithm from NLTK\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "id": "PRObqeMrxIEU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aMCni8kfxXzc",
        "outputId": "44e3ac9f-cc61-4a7a-fe5f-708c9ea45086"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **POS (Part of Speech)**\n",
        "### This code snippet performs Part-of-Speech (POS) tagging on a sample sentence using NLTK.\n",
        "\n",
        "***data ='This is an example of POS tagging. Hope you like it!'***: This line defines a string variable data containing the sample sentence.\n",
        "\n",
        "***pos = nltk.pos_tag(nltk.word_tokenize(data)):*** This is the core of the snippet:\n",
        "\n",
        "***nltk.word_tokenize(data):*** First, the sentence is tokenized into individual words using nltk.word_tokenize().\n",
        "\n",
        "***nltk.pos_tag(...):***  is applied to the list of tokens. This function assigns a grammatical tag (like noun, verb, adjective, etc.) to each word in the list.\n",
        "\n",
        "The result, a list of tuples where each tuple contains a word and its corresponding POS tag, is stored in the pos variable.\n",
        "\n",
        "pos: This line simply displays the pos variable, showing the list of words with their assigned POS tags.\n",
        "\n",
        "###***`POS tagging is a fundamental step in many NLP tasks as it helps in understanding the grammatical structure and meaning of a sentence.`***"
      ],
      "metadata": {
        "id": "02aXVCQayLTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data ='This is an example of POS tagging. Hope you like it!'\n",
        "\n",
        "pos = nltk.pos_tag(nltk.word_tokenize(data))\n",
        "\n",
        "pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5AB7iWDyK37",
        "outputId": "06d09e9d-a6a9-4013-cf2f-4cfbed12f8fa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('an', 'DT'),\n",
              " ('example', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('POS', 'NNP'),\n",
              " ('tagging', 'VBG'),\n",
              " ('.', '.'),\n",
              " ('Hope', 'VBP'),\n",
              " ('you', 'PRP'),\n",
              " ('like', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DT** = determinant | **VBZ**= Verb | **NN**= noun| **PRP**= Proper pronoum\n",
        "**NNP:** Nombre propio, singular\n",
        "**VBG:** Verbo, gerundio o participio presente (por ejemplo, \"tagging\")\n",
        "**N:** Preposición o conjunción subordinada (\"of\", \"in\", \"like\").\n",
        "\n"
      ],
      "metadata": {
        "id": "E2BYx4tSzIhS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WE ALSO HAVE PUNCTUATION WHICH CAN ALSO BE IGNORE\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "punct =string.punctuation\n",
        "punct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ep-geQESzjUw",
        "outputId": "b8d11e07-985a-437c-ce12-6a4c6e3b403b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Punctuation just adds noise to you dataset**\n",
        "\n",
        "### !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`. --> Remove them all!\n",
        "\n",
        "\n",
        "1.   Remove stopwords +\n",
        "2.   Remove punctuation\n",
        "3. **Store this clean data inside** ***\"clean_data\"***\n",
        "\n"
      ],
      "metadata": {
        "id": "i7ZRZeH70DTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's word tokenize the given sample after remove the stopwoRds & punctuation signs\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "punct =string.punctuation\n",
        "\n",
        "data = \"\"\"Stressful times can translate to eating drive-through meals and other packaged, processed items on the go.\n",
        "But those types of foods can make your stress worse, causing a spike and subsequent crash in your blood sugar.\n",
        "Instead of constantly eating out, commit to consuming more home-cooked meals with plenty of fresh fruits and vegetables, whole grains, and lean protein. MDC is a Miami Public collage\"\"\"\n",
        "\n",
        "clean_data =[]\n",
        "\n",
        "for word in nltk.word_tokenize(data):\n",
        "    if word not in punct:\n",
        "        if word not in stop_words:\n",
        "            clean_data.append(word)\n",
        "\n",
        "clean_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ktPMTA840HVI",
        "outputId": "49960393-d12b-41f6-9247-1b47339c102d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Stressful',\n",
              " 'times',\n",
              " 'translate',\n",
              " 'eating',\n",
              " 'drive-through',\n",
              " 'meals',\n",
              " 'packaged',\n",
              " 'processed',\n",
              " 'items',\n",
              " 'go',\n",
              " 'But',\n",
              " 'types',\n",
              " 'foods',\n",
              " 'make',\n",
              " 'stress',\n",
              " 'worse',\n",
              " 'causing',\n",
              " 'spike',\n",
              " 'subsequent',\n",
              " 'crash',\n",
              " 'blood',\n",
              " 'sugar',\n",
              " 'Instead',\n",
              " 'constantly',\n",
              " 'eating',\n",
              " 'commit',\n",
              " 'consuming',\n",
              " 'home-cooked',\n",
              " 'meals',\n",
              " 'plenty',\n",
              " 'fresh',\n",
              " 'fruits',\n",
              " 'vegetables',\n",
              " 'whole',\n",
              " 'grains',\n",
              " 'lean',\n",
              " 'protein',\n",
              " 'MDC',\n",
              " 'Miami',\n",
              " 'Public',\n",
              " 'collage']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**DO A POS (SPEACH TAGGING) IN MY CLEAN_DATA WORDS**\n",
        "\n",
        "### **Name entity recognition (NER):** teach the machine to recognize something specific (example: recognize is this is a person,  celebrity, organization, dates, etc)\n",
        "\n",
        "### This process ***helps to identify and categorize important entities within the text***, which is valuable for information extraction and understanding the content.\n",
        "\n",
        "### ***pos_tag = nltk.pos_tag(clean_data)***: This takes the clean_data list (which presumably contains words after removing stopwords and punctuation) and performs POS tagging on it using nltk.pos_tag(). It assigns a grammatical tag to each word.\n",
        "\n",
        "***namedEntity = nltk.ne_chunk(pos_tag)***: This line takes the list of POS-tagged words (pos_tag) and performs Named Entity Recognition using nltk.ne_chunk(). This function identifies and labels \"named entities\" in the text, such as names of people, organizations, locations, etc.\n",
        "\n",
        "***print(namedEntity)***: This line prints the result of the NER process. The output is a tree-like structure where named entities are grouped and labeled."
      ],
      "metadata": {
        "id": "-DUsXSHG1Phm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# NAMED ENTITY RECOGNITION (NER)\n",
        "# ============================================\n",
        "\n",
        "pos_tag = nltk.pos_tag(clean_data)\n",
        "namedEntity = nltk.ne_chunk(pos_tag)\n",
        "print(namedEntity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qe8ekWoV1dgE",
        "outputId": "13eb6624-88f0-41e4-e1b3-53458d59c03d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  Stressful/JJ\n",
            "  times/NNS\n",
            "  translate/JJ\n",
            "  eating/JJ\n",
            "  drive-through/JJ\n",
            "  meals/NNS\n",
            "  packaged/VBD\n",
            "  processed/JJ\n",
            "  items/NNS\n",
            "  go/VBP\n",
            "  But/CC\n",
            "  types/VBP\n",
            "  foods/NNS\n",
            "  make/VBP\n",
            "  stress/JJ\n",
            "  worse/JJR\n",
            "  causing/VBG\n",
            "  spike/JJ\n",
            "  subsequent/JJ\n",
            "  crash/NN\n",
            "  blood/NN\n",
            "  sugar/NN\n",
            "  Instead/RB\n",
            "  constantly/RB\n",
            "  eating/VBG\n",
            "  commit/NN\n",
            "  consuming/VBG\n",
            "  home-cooked/JJ\n",
            "  meals/NNS\n",
            "  plenty/VBP\n",
            "  fresh/JJ\n",
            "  fruits/NNS\n",
            "  vegetables/NNS\n",
            "  whole/VBP\n",
            "  grains/NNS\n",
            "  lean/JJ\n",
            "  protein/NNS\n",
            "  (ORGANIZATION MDC/NNP Miami/NNP Public/NNP)\n",
            "  collage/NN)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NER without any training could predict/classify that MDC is an organization**.\n",
        "\n",
        "We can train it for something specific\n",
        "\n",
        "**JJ** = is an adJetive"
      ],
      "metadata": {
        "id": "DpkR1TIA2x63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LancasterStemmer() in NLTK is a more aggressive stemming algorithm compared to others like Porter or Snowball.\n",
        "\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} --> {lancaster.stem(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH4jvKEFjN8q",
        "outputId": "8133752c-64fe-4c3e-ae32-498c2e0b698a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And --> and\n",
            "feel --> feel\n",
            "calmer --> calm\n",
            "less --> less\n",
            "screen --> screen\n",
            "time --> tim\n",
            ", --> ,\n",
            "consider --> consid\n",
            "signing --> sign\n",
            "social --> soc\n",
            "medium --> med\n",
            "entirely --> entir\n",
            ". --> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PorterStemmer() - in NLTK is one of the most commonly used stemming algorithms. It's less aggressive than Lancaster and is designed\n",
        "# to strike a balance between reducing words to a common base and maintaining meaning.\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} --> {porter.stem(word)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MekTccXtoGtQ",
        "outputId": "9f89a1f8-4282-4c3e-8ce3-ce0a5b26835c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And --> and\n",
            "feel --> feel\n",
            "calmer --> calmer\n",
            "less --> less\n",
            "screen --> screen\n",
            "time --> time\n",
            ", --> ,\n",
            "consider --> consid\n",
            "signing --> sign\n",
            "social --> social\n",
            "medium --> medium\n",
            "entirely --> entir\n",
            ". --> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SnowballStemmer() - It’s more consistent and linguistically refined — and it supports multiple languages (unlike Porter and Lancaster, which are English-only).\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} --> {snowball.stem(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csZulLNzqDsh",
        "outputId": "c504d4b5-e393-4dce-edea-ffb161cc2cee"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "And --> and\n",
            "feel --> feel\n",
            "calmer --> calmer\n",
            "less --> less\n",
            "screen --> screen\n",
            "time --> time\n",
            ", --> ,\n",
            "consider --> consid\n",
            "signing --> sign\n",
            "social --> social\n",
            "medium --> medium\n",
            "entirely --> entir\n",
            ". --> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk.word_tokenize\n",
        "\n",
        "sent = 'Hello! Iam learning how to use nltk.word_tokenize. It is quite useful'\n",
        "token = list(nltk.word_tokenize(sent))\n",
        "for stemmer in (snowball, lancaster, porter):\n",
        "    stemm = [stemmer.stem(t) for t in token]\n",
        "    print(\" \".join(stemm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33XKycsi7aLV",
        "outputId": "f09d636c-fee8-4b1e-d589-fc50e2a9686f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello ! iam learn how to use nltk.word_token . it is quit use\n",
            "hello ! iam learn how to us nltk.word_tokenize . it is quit us\n",
            "hello ! iam learn how to use nltk.word_token . it is quit use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paragraph\n",
        "\n",
        "paragraph = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. From chatbots to translation apps, NLP is transforming how we interact with technology every day.\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stemming\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDapBfdGAtJm",
        "outputId": "e68e4519-33bc-48b8-ca2a-1cd4b0a4f6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['natur languag process ( nlp ) fascin field combin linguist , comput scienc , artifici intellig .',\n",
              " 'it enabl machin understand , interpret , gener human languag .',\n",
              " 'from chatbot translat app , nlp transform interact technolog everi day .']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PorterSteamer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(stemmer.stem(\"Activation\"))    # Output: \"activ\"\n",
        "print(stemmer.stem(\"Activate\"))      # Output: \"activ\"\n",
        "print(stemmer.stem(\"Activating\"))    # Output: \"activ\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O56fXL4rHEiv",
        "outputId": "7ddb2ec8-8f50-4a7f-92d7-51f23b842327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activ\n",
            "activ\n",
            "activ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem(\"Management\"))  # → \"manag\"\n",
        "print(stemmer.stem(\"Manager\"))     # → \"manag\"\n",
        "print(stemmer.stem(\"Manage\"))      # → \"manag\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OyVv_GFuYv4",
        "outputId": "000f0922-c623-4943-c24c-e9e2ab3fdec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "manag\n",
            "manag\n",
            "manag\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# LEMMATIZATION - The Smart Approach\n",
        "# ============================================\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Stemming (blind chopping)\n",
        "print(stemmer.stem(\"happy\"))           # → \"happy\"\n",
        "print(stemmer.stem(\"happiness\"))       # → \"happiness\"\n",
        "\n",
        "# Lemmatization (understands the relationship!)\n",
        "print(lemmatizer.lemmatize(\"happy\", pos='a'))        # → \"happy\"\n",
        "print(lemmatizer.lemmatize(\"happiness\", pos='a'))    # → \"happiness\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1n_jdw6vigY",
        "outputId": "06da0ca2-9366-4b17-e804-862d44df419a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "happi\n",
            "happi\n",
            "happy\n",
            "happiness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The word \"swam\" - is it a tool or past tense of \"swim\"?\n",
        "\n",
        "# As a verb (past tense of \"swim\")\n",
        "print(lemmatizer.lemmatize(\"swam\", pos='v'))  # → \"swim\"\n",
        "\n",
        "# As a noun (the cutting tool)\n",
        "print(lemmatizer.lemmatize(\"swim\", pos='n'))  # → \"swim\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A11pT4J7xGu1",
        "outputId": "707912e8-4e26-42dd-c906-a1589dfdef27"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swim\n",
            "swim\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('running'))\n",
        "print(lemma.lemmatize('runs'))\n",
        "print(lemma.lemmatize('ran'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xml6QO6iyJr-",
        "outputId": "4e177d3a-9e30-484b-b9ba-e76282d29eaf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\n",
            "run\n",
            "ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatization\n",
        "for i in range(len(sentences)):\n",
        "    words = nltk.word_tokenize(sentences[i])\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i] = ' '.join(words)\n",
        "\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GZDmAOzyUsL",
        "outputId": "e0e095ab-8afc-4f10-d25f-e86c7a2ed00e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['While social medium great way reconnect long-lost friend share personal news milestone , also source negativity information overload .',\n",
              " 'If find getting angry , depressed , stressed scrolling , set social medium limit .',\n",
              " 'Consider deleting apps phone log ’ computer , even temporarily deactivating account .',\n",
              " 'And feel calmer less screen time , consider signing social medium entirely .']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7D73ikqyp8p",
        "outputId": "cbebd7b0-a827-4b0a-82ad-43e8b6f1a814",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# SETUP - Run this first!\n",
        "# ============================================\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "\n",
        "\n",
        "\n",
        "print(\"✅ All NLTK data downloaded successfully!\\n\")\n",
        "\n",
        "# ============================================\n",
        "# PART 1: STEMMING - The Chopping Approach\n",
        "# ============================================\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"🔪 STEMMING EXAMPLES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Initialize stemmers\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer('english')\n",
        "\n",
        "# Words to test\n",
        "words = ['running', 'runs', 'runner', 'ran', 'easily', 'fairly',\n",
        "         'university', 'universe', 'better', 'good']\n",
        "\n",
        "print(\"\\nComparing different stemmers:\")\n",
        "print(f\"{'Word':<12} {'Porter':<12} {'Lancaster':<12} {'Snowball':<12}\")\n",
        "print(\"-\" * 48)\n",
        "\n",
        "for word in words:\n",
        "    p = porter.stem(word)\n",
        "    l = lancaster.stem(word)\n",
        "    s = snowball.stem(word)\n",
        "    print(f\"{word:<12} {p:<12} {l:<12} {s:<12}\")\n",
        "\n",
        "# Show the stemming problem\n",
        "print(\"\\n⚠️  Stemming Problem Example:\")\n",
        "print(f\"'university' → {porter.stem('university')}\")\n",
        "print(f\"'universe'   → {porter.stem('universe')}\")\n",
        "print(\"Both become the same stem even though they're different concepts!\")\n",
        "\n",
        "# ============================================\n",
        "# PART 2: LEMMATIZATION - The Smart Approach\n",
        "# ============================================\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"📚 LEMMATIZATION EXAMPLES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Show how lemmatization handles different word types\n",
        "print(\"\\n1. Irregular verbs (with POS tags):\")\n",
        "irregular_verbs = ['ran', 'ate', 'saw', 'went', 'better', 'best']\n",
        "for word in irregular_verbs:\n",
        "    lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "    print(f\"   {word} → {lemma}\")\n",
        "\n",
        "print(\"\\n2. Irregular plurals:\")\n",
        "irregular_nouns = ['mice', 'geese', 'children', 'feet', 'teeth']\n",
        "for word in irregular_nouns:\n",
        "    lemma = lemmatizer.lemmatize(word, pos='n')\n",
        "    print(f\"   {word} → {lemma}\")\n",
        "\n",
        "print(\"\\n3. Context matters - 'saw' example:\")\n",
        "print(f\"   'saw' as verb → {lemmatizer.lemmatize('saw', pos='v')} (I saw a movie)\")\n",
        "print(f\"   'saw' as noun → {lemmatizer.lemmatize('saw', pos='n')} (I used a saw)\")\n",
        "\n",
        "# ============================================\n",
        "# PART 3: POS TAGGING - Grammar Labels\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🏷️  PART-OF-SPEECH TAGGING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "sentences = [\n",
        "    \"Apple is looking at buying U.K. startup for $1 billion\",\n",
        "    \"I saw her duck under the table\",\n",
        "    \"The complex houses married soldiers and their families\"\n",
        "]\n",
        "\n",
        "for sent in sentences:\n",
        "    tokens = word_tokenize(sent)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    print(f\"\\nSentence: '{sent}'\")\n",
        "    print(\"\\nWord-by-word breakdown:\")\n",
        "    for word, tag in pos_tags:\n",
        "        # Add explanations for common tags\n",
        "        explanations = {\n",
        "            'NN': 'Noun, singular',\n",
        "            'NNS': 'Noun, plural',\n",
        "            'NNP': 'Proper noun, singular',\n",
        "            'VBG': 'Verb, gerund/present participle',\n",
        "            'VBD': 'Verb, past tense',\n",
        "            'VB': 'Verb, base form',\n",
        "            'VBZ': 'Verb, 3rd person singular present',\n",
        "            'JJ': 'Adjective',\n",
        "            'RB': 'Adverb',\n",
        "            'PRP': 'Personal pronoun',\n",
        "            'PRP$': 'Possessive pronoun',\n",
        "            'DT': 'Determiner',\n",
        "            'IN': 'Preposition',\n",
        "            'CC': 'Coordinating conjunction',\n",
        "            'CD': 'Cardinal number',\n",
        "            '.': 'Punctuation'\n",
        "        }\n",
        "        explanation = explanations.get(tag, tag)\n",
        "        print(f\"   {word:<15} → {tag:<6} ({explanation})\")\n",
        "\n",
        "# ============================================\n",
        "# PART 4: STOPWORD REMOVAL\n",
        "# ============================================\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🛑 STOPWORD REMOVAL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text = \"The quick brown fox jumps over the lazy dog\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "filtered = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "print(f\"\\nOriginal: {text}\")\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"After removing stopwords: {filtered}\")\n",
        "print(f\"\\nRemoved words: {[word for word in tokens if word in stop_words]}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 5: NAMED ENTITY RECOGNITION (NER)\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🌟 NAMED ENTITY RECOGNITION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Sample text with various entities\n",
        "ner_text = \"\"\"\n",
        "Barack Obama was born in Hawaii on August 4, 1961.\n",
        "He later worked at Harvard University before joining Google in 2020.\n",
        "Apple Inc. announced a $50 billion investment in Paris, France.\n",
        "The meeting is scheduled for next Monday at 3 PM.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and tag\n",
        "tokens = word_tokenize(ner_text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Perform NER\n",
        "chunks = nltk.ne_chunk(pos_tags, binary=False)\n",
        "\n",
        "print(\"\\nEntities found in the text:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Extract and display entities\n",
        "for chunk in chunks:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        entity_name = ' '.join(c[0] for c in chunk)\n",
        "        entity_type = chunk.label()\n",
        "        print(f\"   {entity_name:<20} → {entity_type}\")\n",
        "\n",
        "# ============================================\n",
        "# PART 6: COMPLETE PIPELINE EXAMPLE\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔄 COMPLETE NLP PIPELINE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Sample document\n",
        "document = \"\"\"\n",
        "Microsoft CEO Satya Nadella announced yesterday that the company\n",
        "is investing heavily in artificial intelligence. The Seattle-based\n",
        "tech giant plans to hire 1000 engineers by December 2024.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"\\nOriginal text:\\n{document}\")\n",
        "\n",
        "# Step 1: Tokenize\n",
        "tokens = word_tokenize(document)\n",
        "print(f\"\\n1. Tokenization: {len(tokens)} tokens\")\n",
        "\n",
        "# Step 2: Remove stopwords\n",
        "filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
        "print(f\"2. After stopword removal: {len(filtered_tokens)} tokens\")\n",
        "\n",
        "# Step 3: Lemmatize\n",
        "lemmatized = [lemmatizer.lemmatize(w.lower()) for w in filtered_tokens]\n",
        "print(f\"3. After lemmatization: {lemmatized[:10]}...\")\n",
        "\n",
        "# Step 4: POS Tag\n",
        "pos_tagged = nltk.pos_tag(tokens)\n",
        "print(f\"4. POS tags: {pos_tagged[:5]}...\")\n",
        "\n",
        "# Step 5: NER\n",
        "ner_chunks = nltk.ne_chunk(pos_tagged)\n",
        "entities = []\n",
        "for chunk in ner_chunks:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        entities.append((' '.join(c[0] for c in chunk), chunk.label()))\n",
        "\n",
        "print(f\"5. Named entities found:\")\n",
        "for entity, label in entities:\n",
        "    print(f\"   - {entity}: {label}\")\n",
        "\n",
        "# ============================================\n",
        "# BONUS: SENTIMENT ANALYSIS\n",
        "# ============================================\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"😊 BONUS: SENTIMENT ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "reviews = [\n",
        "    \"This product is absolutely fantastic! I love it!\",\n",
        "    \"Terrible experience. Would not recommend.\",\n",
        "    \"It's okay, nothing special but does the job.\",\n",
        "    \"The worst purchase I've ever made. Complete waste of money!\"\n",
        "]\n",
        "\n",
        "print(\"\\nAnalyzing sentiments:\")\n",
        "for review in reviews:\n",
        "    scores = sia.polarity_scores(review)\n",
        "    sentiment = 'Positive' if scores['compound'] > 0.05 else 'Negative' if scores['compound'] < -0.05 else 'Neutral'\n",
        "    print(f\"\\n'{review[:50]}...'\")\n",
        "    print(f\"   Sentiment: {sentiment} (score: {scores['compound']:.3f})\")\n",
        "    print(f\"   Details: Pos={scores['pos']:.2f}, Neu={scores['neu']:.2f}, Neg={scores['neg']:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🎉 All examples completed successfully!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km9xA8uLSZVt",
        "outputId": "d6c7350b-ac73-415e-cc6d-fd4c6dd4fab7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All NLTK data downloaded successfully!\n",
            "\n",
            "==================================================\n",
            "🔪 STEMMING EXAMPLES\n",
            "==================================================\n",
            "\n",
            "Comparing different stemmers:\n",
            "Word         Porter       Lancaster    Snowball    \n",
            "------------------------------------------------\n",
            "running      run          run          run         \n",
            "runs         run          run          run         \n",
            "runner       runner       run          runner      \n",
            "ran          ran          ran          ran         \n",
            "easily       easili       easy         easili      \n",
            "fairly       fairli       fair         fair        \n",
            "university   univers      univers      univers     \n",
            "universe     univers      univers      univers     \n",
            "better       better       bet          better      \n",
            "good         good         good         good        \n",
            "\n",
            "⚠️  Stemming Problem Example:\n",
            "'university' → univers\n",
            "'universe'   → univers\n",
            "Both become the same stem even though they're different concepts!\n",
            "\n",
            "==================================================\n",
            "📚 LEMMATIZATION EXAMPLES\n",
            "==================================================\n",
            "\n",
            "1. Irregular verbs (with POS tags):\n",
            "   ran → run\n",
            "   ate → eat\n",
            "   saw → saw\n",
            "   went → go\n",
            "   better → better\n",
            "   best → best\n",
            "\n",
            "2. Irregular plurals:\n",
            "   mice → mouse\n",
            "   geese → goose\n",
            "   children → child\n",
            "   feet → foot\n",
            "   teeth → teeth\n",
            "\n",
            "3. Context matters - 'saw' example:\n",
            "   'saw' as verb → saw (I saw a movie)\n",
            "   'saw' as noun → saw (I used a saw)\n",
            "\n",
            "==================================================\n",
            "🏷️  PART-OF-SPEECH TAGGING\n",
            "==================================================\n",
            "\n",
            "Sentence: 'Apple is looking at buying U.K. startup for $1 billion'\n",
            "\n",
            "Word-by-word breakdown:\n",
            "   Apple           → NNP    (Proper noun, singular)\n",
            "   is              → VBZ    (Verb, 3rd person singular present)\n",
            "   looking         → VBG    (Verb, gerund/present participle)\n",
            "   at              → IN     (Preposition)\n",
            "   buying          → VBG    (Verb, gerund/present participle)\n",
            "   U.K.            → NNP    (Proper noun, singular)\n",
            "   startup         → NN     (Noun, singular)\n",
            "   for             → IN     (Preposition)\n",
            "   $               → $      ($)\n",
            "   1               → CD     (Cardinal number)\n",
            "   billion         → CD     (Cardinal number)\n",
            "\n",
            "Sentence: 'I saw her duck under the table'\n",
            "\n",
            "Word-by-word breakdown:\n",
            "   I               → PRP    (Personal pronoun)\n",
            "   saw             → VBD    (Verb, past tense)\n",
            "   her             → PRP    (Personal pronoun)\n",
            "   duck            → NN     (Noun, singular)\n",
            "   under           → IN     (Preposition)\n",
            "   the             → DT     (Determiner)\n",
            "   table           → NN     (Noun, singular)\n",
            "\n",
            "Sentence: 'The complex houses married soldiers and their families'\n",
            "\n",
            "Word-by-word breakdown:\n",
            "   The             → DT     (Determiner)\n",
            "   complex         → JJ     (Adjective)\n",
            "   houses          → NNS    (Noun, plural)\n",
            "   married         → VBD    (Verb, past tense)\n",
            "   soldiers        → NNS    (Noun, plural)\n",
            "   and             → CC     (Coordinating conjunction)\n",
            "   their           → PRP$   (Possessive pronoun)\n",
            "   families        → NNS    (Noun, plural)\n",
            "\n",
            "==================================================\n",
            "🛑 STOPWORD REMOVAL\n",
            "==================================================\n",
            "\n",
            "Original: The quick brown fox jumps over the lazy dog\n",
            "Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "After removing stopwords: ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
            "\n",
            "Removed words: ['the', 'over', 'the']\n",
            "\n",
            "==================================================\n",
            "🌟 NAMED ENTITY RECOGNITION\n",
            "==================================================\n",
            "\n",
            "Entities found in the text:\n",
            "------------------------------\n",
            "   Barack               → PERSON\n",
            "   Obama                → PERSON\n",
            "   Hawaii               → GPE\n",
            "   Harvard University   → ORGANIZATION\n",
            "   Google               → PERSON\n",
            "   Apple Inc.           → PERSON\n",
            "   Paris                → GPE\n",
            "   France               → GPE\n",
            "\n",
            "==================================================\n",
            "🔄 COMPLETE NLP PIPELINE\n",
            "==================================================\n",
            "\n",
            "Original text:\n",
            "\n",
            "Microsoft CEO Satya Nadella announced yesterday that the company\n",
            "is investing heavily in artificial intelligence. The Seattle-based\n",
            "tech giant plans to hire 1000 engineers by December 2024.\n",
            "\n",
            "\n",
            "1. Tokenization: 29 tokens\n",
            "2. After stopword removal: 22 tokens\n",
            "3. After lemmatization: ['microsoft', 'ceo', 'satya', 'nadella', 'announced', 'yesterday', 'company', 'investing', 'heavily', 'artificial']...\n",
            "4. POS tags: [('Microsoft', 'NNP'), ('CEO', 'NNP'), ('Satya', 'NNP'), ('Nadella', 'NNP'), ('announced', 'VBD')]...\n",
            "5. Named entities found:\n",
            "   - Microsoft: PERSON\n",
            "   - CEO Satya Nadella: ORGANIZATION\n",
            "\n",
            "==================================================\n",
            "😊 BONUS: SENTIMENT ANALYSIS\n",
            "==================================================\n",
            "\n",
            "Analyzing sentiments:\n",
            "\n",
            "'This product is absolutely fantastic! I love it!...'\n",
            "   Sentiment: Positive (score: 0.874)\n",
            "   Details: Pos=0.64, Neu=0.36, Neg=0.00\n",
            "\n",
            "'Terrible experience. Would not recommend....'\n",
            "   Sentiment: Negative (score: -0.638)\n",
            "   Details: Pos=0.00, Neu=0.36, Neg=0.64\n",
            "\n",
            "'It's okay, nothing special but does the job....'\n",
            "   Sentiment: Neutral (score: -0.046)\n",
            "   Details: Pos=0.16, Neu=0.66, Neg=0.18\n",
            "\n",
            "'The worst purchase I've ever made. Complete waste ...'\n",
            "   Sentiment: Negative (score: -0.802)\n",
            "   Details: Pos=0.00, Neu=0.53, Neg=0.47\n",
            "\n",
            "==================================================\n",
            "🎉 All examples completed successfully!\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}
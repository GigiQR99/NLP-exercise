{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GigiQR99/Exercise-Kye/blob/main/Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nl8-GbtHBPL3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "# %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text(sentence):\n",
        "    return re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()"
      ],
      "metadata": {
        "id": "3bhjiDr5HEFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your sentence\n",
        "sentence = '''The class ordered the book from Shark.Pack BUT.It hasn't arrived'''\n",
        "\n",
        "# Call the function\n",
        "result = clean_text(sentence)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvTiVg6nHvd4",
        "outputId": "0005ff1e-9908-4d1b-eb1d-92d255f7cfbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'class', 'ordered', 'the', 'book', 'from', 'Shark', 'Pack', 'BUT', 'It', 'hasn', 't', 'arrived']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def n_gram_extractor(sentence, n):\n",
        "    tokens = re.sub(r'([^\\s\\w]|_)+', ' ', sentence).split()\n",
        "    for i in range(len(tokens)-n+1):\n",
        "        print(tokens[i:i+n])"
      ],
      "metadata": {
        "id": "pNTq6N7mIl99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to see bigrams\n",
        "n_gram_extractor('The cute little boy is playing with the kitten.', 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF1ZISY0Jw5_",
        "outputId": "c67c540d-5457-4333-ed42-253b0a4431fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'cute', 'little']\n",
            "['cute', 'little', 'boy']\n",
            "['little', 'boy', 'is']\n",
            "['boy', 'is', 'playing']\n",
            "['is', 'playing', 'with']\n",
            "['playing', 'with', 'the']\n",
            "['with', 'the', 'kitten']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "list(ngrams('The cute little boy is playing with the kitten.'.split(), 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_RoNy2_KD6p",
        "outputId": "6223c2c9-7a57-4774-e5c2-0b7566e3f992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'cute'),\n",
              " ('cute', 'little'),\n",
              " ('little', 'boy'),\n",
              " ('boy', 'is'),\n",
              " ('is', 'playing'),\n",
              " ('playing', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'kitten.')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from textblob import TextBlob\n",
        "blob = TextBlob(\"The cute little boy is playing with the kitten.\")\n",
        "blob.ngrams(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uaygh7KoKQ7x",
        "outputId": "3b2d79dd-4dcd-4f11-e90d-b12d7eaac4d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['The', 'cute']),\n",
              " WordList(['cute', 'little']),\n",
              " WordList(['little', 'boy']),\n",
              " WordList(['boy', 'is']),\n",
              " WordList(['is', 'playing']),\n",
              " WordList(['playing', 'with']),\n",
              " WordList(['with', 'the']),\n",
              " WordList(['the', 'kitten'])]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C4QD_Z48LR_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip\n",
        "!pip list | grep nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys3V7wzhKxus",
        "outputId": "93ebc8c4-0d76-4898-e6f2-10050f69d873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Usage:   \n",
            "  pip3 <command> [options]\n",
            "\n",
            "Commands:\n",
            "  install                     Install packages.\n",
            "  download                    Download packages.\n",
            "  uninstall                   Uninstall packages.\n",
            "  freeze                      Output installed packages in requirements format.\n",
            "  inspect                     Inspect the python environment.\n",
            "  list                        List installed packages.\n",
            "  show                        Show information about installed packages.\n",
            "  check                       Verify installed packages have compatible dependencies.\n",
            "  config                      Manage local and global configuration.\n",
            "  search                      Search PyPI for packages.\n",
            "  cache                       Inspect and manage pip's wheel cache.\n",
            "  index                       Inspect information available from package indexes.\n",
            "  wheel                       Build wheels from your requirements.\n",
            "  hash                        Compute hashes of package archives.\n",
            "  completion                  A helper command used for command completion.\n",
            "  debug                       Show information useful for debugging.\n",
            "  help                        Show help for commands.\n",
            "\n",
            "General Options:\n",
            "  -h, --help                  Show help.\n",
            "  --debug                     Let unhandled exceptions propagate outside the\n",
            "                              main subroutine, instead of logging them to\n",
            "                              stderr.\n",
            "  --isolated                  Run pip in an isolated mode, ignoring\n",
            "                              environment variables and user configuration.\n",
            "  --require-virtualenv        Allow pip to only run in a virtual environment;\n",
            "                              exit with an error otherwise.\n",
            "  --python <python>           Run pip with the specified Python interpreter.\n",
            "  -v, --verbose               Give more output. Option is additive, and can be\n",
            "                              used up to 3 times.\n",
            "  -V, --version               Show version and exit.\n",
            "  -q, --quiet                 Give less output. Option is additive, and can be\n",
            "                              used up to 3 times (corresponding to WARNING,\n",
            "                              ERROR, and CRITICAL logging levels).\n",
            "  --log <path>                Path to a verbose appending log.\n",
            "  --no-input                  Disable prompting for input.\n",
            "  --keyring-provider <keyring_provider>\n",
            "                              Enable the credential lookup via the keyring\n",
            "                              library if user input is allowed. Specify which\n",
            "                              mechanism to use [disabled, import, subprocess].\n",
            "                              (default: disabled)\n",
            "  --proxy <proxy>             Specify a proxy in the form\n",
            "                              scheme://[user:passwd@]proxy.server:port.\n",
            "  --retries <retries>         Maximum number of retries each connection should\n",
            "                              attempt (default 5 times).\n",
            "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
            "  --exists-action <action>    Default action when a path already exists:\n",
            "                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n",
            "  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n",
            "                              even though it does not have valid or any HTTPS.\n",
            "  --cert <path>               Path to PEM-encoded CA certificate bundle. If\n",
            "                              provided, overrides the default. See 'SSL\n",
            "                              Certificate Verification' in pip documentation\n",
            "                              for more information.\n",
            "  --client-cert <path>        Path to SSL client certificate, a single file\n",
            "                              containing the private key and the certificate\n",
            "                              in PEM format.\n",
            "  --cache-dir <dir>           Store the cache data in <dir>.\n",
            "  --no-cache-dir              Disable the cache.\n",
            "  --disable-pip-version-check\n",
            "                              Don't periodically check PyPI to determine\n",
            "                              whether a new version of pip is available for\n",
            "                              download. Implied with --no-index.\n",
            "  --no-color                  Suppress colored output.\n",
            "  --no-python-version-warning\n",
            "                              Silence deprecation warnings for upcoming\n",
            "                              unsupported Pythons.\n",
            "  --use-feature <feature>     Enable new functionality, that may be backward\n",
            "                              incompatible.\n",
            "  --use-deprecated <feature>  Enable deprecated functionality, that will be\n",
            "                              removed in the future.\n",
            "nltk                                  3.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from textblob import TextBlob\n",
        "\n",
        "sentence = 'Carlos posted, \"Watching Super Bowl LVIII from Hard Rock Stadium, \\\n",
        "Miami Gardens. Amazing halftimeby Shakira! Incredible atmosphere! @miamidolphins \\\n",
        "@nfl #Miami #SuperBowl2024. For event photos contact me carlos@miamishots.com :)\"'"
      ],
      "metadata": {
        "id": "LJCPiYu_LFtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keras_tokens(text):\n",
        "    return text_to_word_sequence(text)\n",
        "\n",
        "# Run this to see Keras output\n",
        "result_keras = get_keras_tokens(sentence)\n",
        "print(result_keras)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJlWVnwtMGKd",
        "outputId": "fccb753c-0f1a-45a6-e3df-259f50ed384e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['carlos', 'posted', 'watching', 'super', 'bowl', 'lviii', 'from', 'hard', 'rock', 'stadium', 'miami', 'gardens', 'amazing', 'halftimeby', 'shakira', 'incredible', 'atmosphere', 'miamidolphins', 'nfl', 'miami', 'superbowl2024', 'for', 'event', 'photos', 'contact', 'me', 'carlos', 'miamishots', 'com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_textblob_tokens(text):\n",
        "    blob = TextBlob(text)\n",
        "    return blob.words\n",
        "\n",
        "# Run this to see TextBlob output\n",
        "result_textblob = get_textblob_tokens(sentence)\n",
        "print(result_textblob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuV6r3bUMWZK",
        "outputId": "363b6f80-c1ca-46be-b3c2-ae639aa5cfe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'posted', 'Watching', 'Super', 'Bowl', 'LVIII', 'from', 'Hard', 'Rock', 'Stadium', 'Miami', 'Gardens', 'Amazing', 'halftimeby', 'Shakira', 'Incredible', 'atmosphere', 'miamidolphins', 'nfl', 'Miami', 'SuperBowl2024', 'For', 'event', 'photos', 'contact', 'me', 'carlos', 'miamishots.com']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to see both outputs side by side\n",
        "sentence = 'Carlos posted, \"Watching Super Bowl LVIII from Hard Rock Stadium, \\\n",
        "Miami Gardens. Amazing halftimeby Shakira! Incredible atmosphere! @miamidolphins \\\n",
        "@nfl #Miami #SuperBowl2024. For event photos contact me carlos@miamishots.com :)\"'\n",
        "\n",
        "print(\"KERAS TOKENS:\")\n",
        "keras_tokens = get_keras_tokens(sentence)\n",
        "print(keras_tokens)\n",
        "print(f\"Token count: {len(keras_tokens)}\")\n",
        "\n",
        "print(\"\\nTEXTBLOB TOKENS:\")\n",
        "textblob_tokens = get_textblob_tokens(sentence)\n",
        "print(list(textblob_tokens))\n",
        "print(f\"Token count: {len(textblob_tokens)}\")\n",
        "\n",
        "print(\"\\nDIFFERENCES:\")\n",
        "print(f\"Email handling: Keras splits it, TextBlob keeps it whole\")\n",
        "print(f\"Hashtags: Keras removes #, TextBlob preserves #\")\n",
        "print(f\"Mentions: Keras removes @, TextBlob preserves @\")\n",
        "print(f\"Case: Keras lowercases, TextBlob preserves case\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1ZrfL7hMnh-",
        "outputId": "c5b03862-3de9-473b-b3eb-4d454295665f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KERAS TOKENS:\n",
            "['carlos', 'posted', 'watching', 'super', 'bowl', 'lviii', 'from', 'hard', 'rock', 'stadium', 'miami', 'gardens', 'amazing', 'halftimeby', 'shakira', 'incredible', 'atmosphere', 'miamidolphins', 'nfl', 'miami', 'superbowl2024', 'for', 'event', 'photos', 'contact', 'me', 'carlos', 'miamishots', 'com']\n",
            "Token count: 29\n",
            "\n",
            "TEXTBLOB TOKENS:\n",
            "['Carlos', 'posted', 'Watching', 'Super', 'Bowl', 'LVIII', 'from', 'Hard', 'Rock', 'Stadium', 'Miami', 'Gardens', 'Amazing', 'halftimeby', 'Shakira', 'Incredible', 'atmosphere', 'miamidolphins', 'nfl', 'Miami', 'SuperBowl2024', 'For', 'event', 'photos', 'contact', 'me', 'carlos', 'miamishots.com']\n",
            "Token count: 28\n",
            "\n",
            "DIFFERENCES:\n",
            "Email handling: Keras splits it, TextBlob keeps it whole\n",
            "Hashtags: Keras removes #, TextBlob preserves #\n",
            "Mentions: Keras removes @, TextBlob preserves @\n",
            "Case: Keras lowercases, TextBlob preserves case\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Analyzing Miami tourism tweets\n",
        "miami_tweet = \"Just visited @ViscayaMuseum! Beautiful gardens ðŸŒº #MiamiDade #ArtDeco. Book tours at info@viscaya.org\"\n",
        "\n",
        "print(\"For hashtag analysis (use TextBlob):\")\n",
        "print(get_textblob_tokens(miami_tweet))  # Preserves #MiamiDade, #ArtDeco\n",
        "\n",
        "print(\"\\nFor word frequency analysis (use Keras):\")\n",
        "print(get_keras_tokens(miami_tweet))  # Normalizes everything for counting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch2o_o_HM4yX",
        "outputId": "554ebe8b-6b22-4a93-8378-9f744f1c47dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For hashtag analysis (use TextBlob):\n",
            "['Just', 'visited', 'ViscayaMuseum', 'Beautiful', 'gardens', 'ðŸŒº', 'MiamiDade', 'ArtDeco', 'Book', 'tours', 'at', 'info', 'viscaya.org']\n",
            "\n",
            "For word frequency analysis (use Keras):\n",
            "['just', 'visited', 'viscayamuseum', 'beautiful', 'gardens', 'ðŸŒº', 'miamidade', 'artdeco', 'book', 'tours', 'at', 'info', 'viscaya', 'org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "sentence = 'Carlos tweeted, \"Experiencing Ultra Music Festival from Bayfront Park, \\\n",
        "Miami. Incredible performance by Swedish House Mafia! Amazing visuals! @ultra \\\n",
        "@marshmello #Miami #UltraFest2024. For VIP tickets contact carlos@ultramiami.com :)\"'"
      ],
      "metadata": {
        "id": "sRgOWw9ZOZYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_tweet_tokenizer(text):\n",
        "    tweet_tokenizer = TweetTokenizer()\n",
        "    return tweet_tokenizer.tokenize(text)\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_tweet_tokenizer(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qQUGzrrOnTz",
        "outputId": "5d7b8075-accc-4e38-826f-a317371a4fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted', ',', '\"', 'Experiencing', 'Ultra', 'Music', 'Festival', 'from', 'Bayfront', 'Park', ',', 'Miami', '.', 'Incredible', 'performance', 'by', 'Swedish', 'House', 'Mafia', '!', 'Amazing', 'visuals', '!', '@ultra', '@marshmello', '#Miami', '#UltraFest2024', '.', 'For', 'VIP', 'tickets', 'contact', 'carlos@ultramiami.com', ':)', '\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_mwe(text):\n",
        "    mwe_tokenizer = MWETokenizer([('Bayfront', 'Park')])  # Define multi-word expressions\n",
        "    mwe_tokenizer.add_mwe(('Swedish', 'House', 'Mafia!'))  # Add another MWE\n",
        "    mwe_tokenizer.add_mwe(('Ultra', 'Music', 'Festival'))\n",
        "    return mwe_tokenizer.tokenize(text.split())\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_mwe(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjZSZgaVO8qT",
        "outputId": "90ff6fca-9ff3-4cf4-afb8-32e10acadf65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted,', '\"Experiencing', 'Ultra_Music_Festival', 'from', 'Bayfront', 'Park,', 'Miami.', 'Incredible', 'performance', 'by', 'Swedish_House_Mafia!', 'Amazing', 'visuals!', '@ultra', '@marshmello', '#Miami', '#UltraFest2024.', 'For', 'VIP', 'tickets', 'contact', 'carlos@ultramiami.com', ':)\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_regex_tokenizer(text):\n",
        "    # Pattern: \\w+ (words) | \\$[\\d\\.]+ (prices) | \\S+ (non-spaces)\n",
        "    reg_tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
        "    return reg_tokenizer.tokenize(text)\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_regex_tokenizer(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zeug5pGPZxo",
        "outputId": "77f71728-4e38-4801-fc50-5e689da9a347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted', ',', '\"Experiencing', 'Ultra', 'Music', 'Festival', 'from', 'Bayfront', 'Park', ',', 'Miami', '.', 'Incredible', 'performance', 'by', 'Swedish', 'House', 'Mafia', '!', 'Amazing', 'visuals', '!', '@ultra', '@marshmello', '#Miami', '#UltraFest2024.', 'For', 'VIP', 'tickets', 'contact', 'carlos', '@ultramiami.com', ':)\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_wst(text):\n",
        "    wh_tokenizer = WhitespaceTokenizer()\n",
        "    return wh_tokenizer.tokenize(text)\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_wst(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7PdhhroQd2S",
        "outputId": "61679362-fe29-459d-e7e3-85b2948977f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted,', '\"Experiencing', 'Ultra', 'Music', 'Festival', 'from', 'Bayfront', 'Park,', 'Miami.', 'Incredible', 'performance', 'by', 'Swedish', 'House', 'Mafia!', 'Amazing', 'visuals!', '@ultra', '@marshmello', '#Miami', '#UltraFest2024.', 'For', 'VIP', 'tickets', 'contact', 'carlos@ultramiami.com', ':)\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_with_wordpunct_tokenizer(text):\n",
        "    wp_tokenizer = WordPunctTokenizer()\n",
        "    return wp_tokenizer.tokenize(text)\n",
        "\n",
        "# Run this\n",
        "result = tokenize_with_wordpunct_tokenizer(sentence)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoZq_WXcQzVT",
        "outputId": "2541a24f-7682-4ab8-a6e5-22cefc9d9e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Carlos', 'tweeted', ',', '\"', 'Experiencing', 'Ultra', 'Music', 'Festival', 'from', 'Bayfront', 'Park', ',', 'Miami', '.', 'Incredible', 'performance', 'by', 'Swedish', 'House', 'Mafia', '!', 'Amazing', 'visuals', '!', '@', 'ultra', '@', 'marshmello', '#', 'Miami', '#', 'UltraFest2024', '.', 'For', 'VIP', 'tickets', 'contact', 'carlos', '@', 'ultramiami', '.', 'com', ':)\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing Miami restaurant reviews with different needs\n",
        "review = \"Amazing dinner @JoesStone! Stone crabs = $49.95. Must-try! #MiamiFood :)\"\n",
        "\n",
        "# For sentiment with emoticons\n",
        "print(\"For sentiment analysis:\")\n",
        "print(tokenize_with_tweet_tokenizer(review))\n",
        "\n",
        "# For price extraction\n",
        "print(\"\\nFor price extraction:\")\n",
        "price_tokenizer = RegexpTokenizer('\\$[\\d\\.]+|\\w+|\\S+')\n",
        "print(price_tokenizer.tokenize(review))\n",
        "\n",
        "# For keeping restaurant names\n",
        "print(\"\\nFor entity recognition:\")\n",
        "mwe = MWETokenizer([('Stone', 'crabs')])\n",
        "print(mwe.tokenize(review.split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUW7hqCoQ6k4",
        "outputId": "8c17f76d-8c12-4a32-94bf-bb5e339be7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For sentiment analysis:\n",
            "['Amazing', 'dinner', '@JoesStone', '!', 'Stone', 'crabs', '=', '$', '49.95', '.', 'Must-try', '!', '#MiamiFood', ':)']\n",
            "\n",
            "For price extraction:\n",
            "['Amazing', 'dinner', '@JoesStone!', 'Stone', 'crabs', '=', '$49.95.', 'Must', '-try!', '#MiamiFood', ':)']\n",
            "\n",
            "For entity recognition:\n",
            "['Amazing', 'dinner', '@JoesStone!', 'Stone_crabs', '=', '$49.95.', 'Must-try!', '#MiamiFood', ':)']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:10: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:10: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-535133730.py:10: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  price_tokenizer = RegexpTokenizer('\\$[\\d\\.]+|\\w+|\\S+')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "def get_stems(text):\n",
        "    regex_stemmer = RegexpStemmer('ing$', min=4) # creating an object of RegexpStemmer,\n",
        "                                             # any string ending with the given\n",
        "                                             # regex â€˜ing$â€™ will be removed.\n",
        "    # The below code line will convert every word into its stem using regex stemmer\n",
        "    # and then join them with space.\n",
        "    return ' '.join([regex_stemmer.stem(wd) for wd in text.split()])\n",
        "\n",
        "\n",
        "sentence = \"I love playing football\"\n",
        "get_stems(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dQhsp4I4Sqo-",
        "outputId": "a584de8a-4d43-4ba0-e796-efb9f7bc10d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I love play football'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "\n",
        "sentence = \"Before eating it would be nice to sanitize your hands with a sanitizer\"\n",
        "\n",
        "def get_stems(text):\n",
        "    ps_stemmer = PorterStemmer()\n",
        "    return ' '.join([ps_stemmer.stem(wd) for wd in text.split()])\n",
        "\n",
        "get_stems(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pzWPq_rtS1Nf",
        "outputId": "b8b10570-7dbc-4fc8-be95-434a50252b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'befor eat it would be nice to sanit your hand with a sanit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "sentence = \"The products produced by the process today are far better than what it produces generally.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2tOqwCUUH_t",
        "outputId": "a301f974-c555-4565-b835-28758f1ebdf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def get_lemmas(text):\n",
        "    return ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(text)])\n",
        "\n",
        "\n",
        "\n",
        "get_lemmas(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1fmu67xmUR4b",
        "outputId": "929de10b-44ec-4c32-b9d7-9dd2f7de2f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The product produced by the process today are far better than what it produce generally .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "sentence = TextBlob('She sells seashells on the seashore')"
      ],
      "metadata": {
        "id": "vdv_Dy41UfiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBkBGOhnUhyJ",
        "outputId": "4a323fc1-78fd-48d9-8a75-75cd02449ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['She', 'sells', 'seashells', 'on', 'the', 'seashore'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def singularize(word):\n",
        "    return word.singularize()\n",
        "\n",
        "singularize(sentence.words[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9InZMhuFUj8J",
        "outputId": "65ea81cb-4363-4163-9bc8-3cd7afb25224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'seashell'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pluralize(word):\n",
        "    return word.pluralize()\n",
        "\n",
        "pluralize(sentence.words[5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "61dHSIgvUq_W",
        "outputId": "48e042f0-d1d6-4150-9bbc-90cca1385fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'seashores'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c895f1c",
        "outputId": "fecf943e-4181-4729-afa9-8ed14a74afc5"
      },
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0667d52a",
        "outputId": "5394fc1d-2d45-49fb-f3a0-b70041768187"
      },
      "source": [
        "!pip install textblob[translate]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob[translate] in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "\u001b[33mWARNING: textblob 0.19.0 does not provide the extra 'translate'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob[translate]) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob[translate]) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob[translate]) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob[translate]) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob[translate]) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative using googletrans\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "def translate_alt(text, from_l, to_l):\n",
        "    translator = Translator()\n",
        "    result = translator.translate(text, src=from_l, dest=to_l)\n",
        "    return result.text\n",
        "\n",
        "# Test\n",
        "result = translate_alt(text='por favor', from_l='es', to_l='en')\n",
        "print(result)  # Should output: \"very good\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUedWwdxU4I7",
        "outputId": "a5b2e71b-49f4-459e-aa83-aa36e533fda5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.12/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.12/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.8.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.12/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.12/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.12/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.12/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
            "please\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "SpaCy Text Cleaning Proof of Concept\n",
        "=====================================\n",
        "A comprehensive example showing various text cleaning techniques using SpaCy.\n",
        "\"\"\"\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import re\n",
        "from typing import List, Set\n",
        "\n",
        "\n",
        "class TextCleaner:\n",
        "    \"\"\"A comprehensive text cleaning pipeline using SpaCy.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"en_core_web_sm\"):\n",
        "        \"\"\"\n",
        "        Initialize the text cleaner with a SpaCy model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the SpaCy model to load\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.nlp = spacy.load(model_name)\n",
        "        except OSError:\n",
        "            print(f\"Model '{model_name}' not found. Installing...\")\n",
        "            import subprocess\n",
        "            subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", model_name])\n",
        "            self.nlp = spacy.load(model_name)\n",
        "\n",
        "        # Get stop words from SpaCy\n",
        "        self.stop_words = STOP_WORDS\n",
        "\n",
        "    def clean_text(self,\n",
        "                   text: str,\n",
        "                   lowercase: bool = True,\n",
        "                   remove_stopwords: bool = True,\n",
        "                   remove_punctuation: bool = True,\n",
        "                   remove_numbers: bool = False,\n",
        "                   remove_spaces: bool = True,\n",
        "                   lemmatize: bool = True,\n",
        "                   remove_emails: bool = True,\n",
        "                   remove_urls: bool = True,\n",
        "                   remove_special_chars: bool = True,\n",
        "                   min_token_length: int = 2) -> str:\n",
        "        \"\"\"\n",
        "        Clean text using various techniques.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to clean\n",
        "            lowercase: Convert to lowercase\n",
        "            remove_stopwords: Remove stop words\n",
        "            remove_punctuation: Remove punctuation\n",
        "            remove_numbers: Remove numeric tokens\n",
        "            remove_spaces: Remove extra whitespace\n",
        "            lemmatize: Convert words to lemmas\n",
        "            remove_emails: Remove email addresses\n",
        "            remove_urls: Remove URLs\n",
        "            remove_special_chars: Remove special characters\n",
        "            min_token_length: Minimum token length to keep\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "\n",
        "        # Pre-processing: Remove URLs and emails using regex\n",
        "        if remove_urls:\n",
        "            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "            text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "        if remove_emails:\n",
        "            text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Process with SpaCy\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Token-level cleaning\n",
        "        cleaned_tokens = []\n",
        "\n",
        "        for token in doc:\n",
        "            # Skip based on various criteria\n",
        "            if remove_punctuation and token.is_punct:\n",
        "                continue\n",
        "\n",
        "            if remove_stopwords and token.text.lower() in self.stop_words:\n",
        "                continue\n",
        "\n",
        "            if remove_numbers and (token.like_num or token.is_digit):\n",
        "                continue\n",
        "\n",
        "            if token.is_space:\n",
        "                continue\n",
        "\n",
        "            # Get the token text (lemma or original)\n",
        "            if lemmatize and not token.is_punct:\n",
        "                token_text = token.lemma_\n",
        "            else:\n",
        "                token_text = token.text\n",
        "\n",
        "            # Apply lowercase\n",
        "            if lowercase:\n",
        "                token_text = token_text.lower()\n",
        "\n",
        "            # Check minimum length\n",
        "            if len(token_text) < min_token_length:\n",
        "                continue\n",
        "\n",
        "            # Remove special characters if requested\n",
        "            if remove_special_chars:\n",
        "                token_text = re.sub(r'[^a-zA-Z0-9\\s]', '', token_text)\n",
        "\n",
        "            # Add to cleaned tokens if not empty\n",
        "            if token_text.strip():\n",
        "                cleaned_tokens.append(token_text)\n",
        "\n",
        "        # Join tokens\n",
        "        cleaned_text = ' '.join(cleaned_tokens)\n",
        "\n",
        "        # Remove extra spaces\n",
        "        if remove_spaces:\n",
        "            cleaned_text = ' '.join(cleaned_text.split())\n",
        "\n",
        "        return cleaned_text\n",
        "\n",
        "    def extract_entities(self, text: str) -> List[tuple]:\n",
        "        \"\"\"\n",
        "        Extract named entities from text.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            List of (entity_text, entity_label) tuples\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    def get_pos_tags(self, text: str) -> List[tuple]:\n",
        "        \"\"\"\n",
        "        Get part-of-speech tags for tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            List of (token, pos_tag) tuples\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "    def remove_by_pos(self, text: str, pos_to_remove: Set[str]) -> str:\n",
        "        \"\"\"\n",
        "        Remove tokens based on their POS tags.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            pos_to_remove: Set of POS tags to remove (e.g., {'ADV', 'ADJ'})\n",
        "\n",
        "        Returns:\n",
        "            Cleaned text\n",
        "        \"\"\"\n",
        "        doc = self.nlp(text)\n",
        "        tokens = [token.text for token in doc if token.pos_ not in pos_to_remove]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def normalize_whitespace(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize various types of whitespace characters.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "\n",
        "        Returns:\n",
        "            Text with normalized whitespace\n",
        "        \"\"\"\n",
        "        # Replace various whitespace characters with regular space\n",
        "        text = re.sub(r'[\\t\\n\\r\\f\\v]+', ' ', text)\n",
        "        # Remove multiple spaces\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Demonstrate the text cleaning capabilities.\"\"\"\n",
        "\n",
        "    # Sample paragraph with various issues\n",
        "    sample_text = \"\"\"\n",
        "    Hello World!!!  This is a SAMPLE paragraph with     various issues.\n",
        "    Visit our website at https://www.example.com or contact us at info@example.com.\n",
        "\n",
        "    The meeting is scheduled for 15th December, 2024 at 3:30 PM.\n",
        "    We'll be discussing the Q4 2024 results and planning for 2025.\n",
        "\n",
        "    Some unnecessary words: very, really, actually, basically are often overused.\n",
        "    Numbers like 123, 456.78 and special characters @#$% should be handled!\n",
        "\n",
        "    Microsoft Corporation (MSFT) announced new features.   Apple Inc. is also innovating.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SPACY TEXT CLEANING PROOF OF CONCEPT\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Initialize cleaner\n",
        "    cleaner = TextCleaner()\n",
        "\n",
        "    print(\"\\nORIGINAL TEXT:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(sample_text)\n",
        "\n",
        "    # 1. Basic cleaning\n",
        "    print(\"\\n1. BASIC CLEANING (lowercase, punctuation, stopwords):\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        lowercase=True,\n",
        "        remove_stopwords=True,\n",
        "        remove_punctuation=True,\n",
        "        lemmatize=False\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 2. Advanced cleaning with lemmatization\n",
        "    print(\"\\n2. WITH LEMMATIZATION:\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        lowercase=True,\n",
        "        remove_stopwords=True,\n",
        "        remove_punctuation=True,\n",
        "        lemmatize=True\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 3. Removing URLs, emails, and numbers\n",
        "    print(\"\\n3. REMOVING URLs, EMAILS, AND NUMBERS:\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        remove_urls=True,\n",
        "        remove_emails=True,\n",
        "        remove_numbers=True,\n",
        "        lemmatize=True\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 4. Minimal cleaning (preserve more information)\n",
        "    print(\"\\n4. MINIMAL CLEANING (preserve case and some punctuation):\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        lowercase=False,\n",
        "        remove_stopwords=True,\n",
        "        remove_punctuation=False,\n",
        "        lemmatize=False,\n",
        "        remove_urls=True,\n",
        "        remove_emails=True\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 5. Extract named entities\n",
        "    print(\"\\n5. NAMED ENTITIES FOUND:\")\n",
        "    print(\"-\" * 40)\n",
        "    entities = cleaner.extract_entities(sample_text)\n",
        "    for text, label in entities:\n",
        "        print(f\"  - {text}: {label}\")\n",
        "\n",
        "    # 6. Remove specific POS tags\n",
        "    print(\"\\n6. REMOVE ADJECTIVES AND ADVERBS:\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.remove_by_pos(sample_text, {'ADJ', 'ADV'})\n",
        "    print(cleaned)\n",
        "\n",
        "    # 7. Custom cleaning configuration\n",
        "    print(\"\\n7. CUSTOM CONFIGURATION (strict cleaning):\")\n",
        "    print(\"-\" * 40)\n",
        "    cleaned = cleaner.clean_text(\n",
        "        sample_text,\n",
        "        lowercase=True,\n",
        "        remove_stopwords=True,\n",
        "        remove_punctuation=True,\n",
        "        remove_numbers=True,\n",
        "        remove_spaces=True,\n",
        "        lemmatize=True,\n",
        "        remove_emails=True,\n",
        "        remove_urls=True,\n",
        "        remove_special_chars=True,\n",
        "        min_token_length=3  # Only keep tokens with 3+ characters\n",
        "    )\n",
        "    print(cleaned)\n",
        "\n",
        "    # 8. Show POS tags for understanding\n",
        "    print(\"\\n8. PART-OF-SPEECH TAGS (first 20 tokens):\")\n",
        "    print(\"-\" * 40)\n",
        "    pos_tags = cleaner.get_pos_tags(sample_text)[:20]\n",
        "    for token, pos in pos_tags:\n",
        "        if not token.isspace():\n",
        "            print(f\"  {token:15} -> {pos}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"CLEANING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5AX864LWXiE",
        "outputId": "adb39263-14e4-49d3-adf2-4c358b4ef819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "SPACY TEXT CLEANING PROOF OF CONCEPT\n",
            "================================================================================\n",
            "\n",
            "ORIGINAL TEXT:\n",
            "----------------------------------------\n",
            "\n",
            "    Hello World!!!  This is a SAMPLE paragraph with     various issues.\n",
            "    Visit our website at https://www.example.com or contact us at info@example.com.\n",
            "    \n",
            "    The meeting is scheduled for 15th December, 2024 at 3:30 PM.\n",
            "    We'll be discussing the Q4 2024 results and planning for 2025.\n",
            "    \n",
            "    Some unnecessary words: very, really, actually, basically are often overused.\n",
            "    Numbers like 123, 456.78 and special characters @#$% should be handled!\n",
            "    \n",
            "    Microsoft Corporation (MSFT) announced new features.   Apple Inc. is also innovating.\n",
            "    \n",
            "\n",
            "1. BASIC CLEANING (lowercase, punctuation, stopwords):\n",
            "----------------------------------------\n",
            "hello world sample paragraph issues visit website contact meeting scheduled 15th december 2024 330 pm discussing q4 2024 results planning 2025 unnecessary words actually basically overused numbers like 123 45678 special characters handled microsoft corporation msft announced new features apple inc innovating\n",
            "\n",
            "2. WITH LEMMATIZATION:\n",
            "----------------------------------------\n",
            "hello world sample paragraph issue visit website contact meeting schedule 15th december 2024 330 pm discuss q4 2024 result planning 2025 unnecessary word actually basically overused number like 123 45678 special character handle microsoft corporation msft announce new feature apple inc innovate\n",
            "\n",
            "3. REMOVING URLs, EMAILS, AND NUMBERS:\n",
            "----------------------------------------\n",
            "hello world sample paragraph issue visit website contact meeting schedule december 330 pm discuss q4 result planning unnecessary word actually basically overused number like special character handle microsoft corporation msft announce new feature apple inc innovate\n",
            "\n",
            "4. MINIMAL CLEANING (preserve case and some punctuation):\n",
            "----------------------------------------\n",
            "Hello World SAMPLE paragraph issues Visit website contact meeting scheduled 15th December 2024 330 PM discussing Q4 2024 results planning 2025 unnecessary words actually basically overused Numbers like 123 45678 special characters handled Microsoft Corporation MSFT announced new features Apple Inc innovating\n",
            "\n",
            "5. NAMED ENTITIES FOUND:\n",
            "----------------------------------------\n",
            "  - 15th December, 2024: DATE\n",
            "  - 3:30 PM: TIME\n",
            "  - Q4 2024: DATE\n",
            "  - 2025: DATE\n",
            "  - 123: CARDINAL\n",
            "  - 456.78: DATE\n",
            "  - Microsoft Corporation: ORG\n",
            "  - Apple Inc.: ORG\n",
            "\n",
            "6. REMOVE ADJECTIVES AND ADVERBS:\n",
            "----------------------------------------\n",
            "\n",
            "     Hello World ! ! !   This is a SAMPLE paragraph with      issues . \n",
            "     Visit our website at https://www.example.com or contact us at info@example.com . \n",
            "    \n",
            "     The meeting is scheduled for December , 2024 at 3:30 PM . \n",
            "     We 'll be discussing the Q4 2024 results and planning for 2025 . \n",
            "    \n",
            "     Some words : , , , are . \n",
            "     Numbers like 123 , 456.78 and characters @#$% should be handled ! \n",
            "    \n",
            "     Microsoft Corporation ( MSFT ) announced features .    Apple Inc. is innovating . \n",
            "    \n",
            "\n",
            "7. CUSTOM CONFIGURATION (strict cleaning):\n",
            "----------------------------------------\n",
            "hello world sample paragraph issue visit website contact meeting schedule december 330 discuss result planning unnecessary word actually basically overused number like special character handle microsoft corporation msft announce new feature apple inc innovate\n",
            "\n",
            "8. PART-OF-SPEECH TAGS (first 20 tokens):\n",
            "----------------------------------------\n",
            "  Hello           -> INTJ\n",
            "  World           -> PROPN\n",
            "  !               -> PUNCT\n",
            "  !               -> PUNCT\n",
            "  !               -> PUNCT\n",
            "  This            -> PRON\n",
            "  is              -> AUX\n",
            "  a               -> DET\n",
            "  SAMPLE          -> PROPN\n",
            "  paragraph       -> NOUN\n",
            "  with            -> ADP\n",
            "  various         -> ADJ\n",
            "  issues          -> NOUN\n",
            "  .               -> PUNCT\n",
            "  Visit           -> VERB\n",
            "  our             -> PRON\n",
            "\n",
            "================================================================================\n",
            "CLEANING COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "sentence = \"She sells seashells on the seashore\""
      ],
      "metadata": {
        "id": "4C2kTVqkdlbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stop_words(text,stop_word_list):\n",
        "    return ' '.join([word for word in word_tokenize(text) if word.lower() not in stop_word_list])\n",
        "\n",
        "\n",
        "custom_stop_word_list = ['she', 'on', 'the', 'am', 'is', 'not']\n",
        "remove_stop_words(sentence,custom_stop_word_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JJh6oQXvdtH0",
        "outputId": "f57f5fea-ca9f-4cec-99c4-c16d4faf53ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sells seashells seashore'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "df = pd.DataFrame([['The interim budget for 2019 will be announced on 1st February.'], ['Do you know how much expectation the middle-class working population is having from this budget?'], ['February is the shortest month in a year.'], ['This financial year will end on 31st March.']])\n",
        "df.columns = ['text']\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "5g-UsVhmehll",
        "outputId": "376454e1-075c-4040-be52-498878f9883e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text\n",
              "0  The interim budget for 2019 will be announced ...\n",
              "1  Do you know how much expectation the middle-cl...\n",
              "2          February is the shortest month in a year.\n",
              "3        This financial year will end on 31st March."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ee218b0-a1a5-441f-8bf5-553fc07e8ad7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The interim budget for 2019 will be announced ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Do you know how much expectation the middle-cl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>February is the shortest month in a year.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This financial year will end on 31st March.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ee218b0-a1a5-441f-8bf5-553fc07e8ad7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ee218b0-a1a5-441f-8bf5-553fc07e8ad7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ee218b0-a1a5-441f-8bf5-553fc07e8ad7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e3d28d62-0810-4a22-b9ef-7dd2fa1d3721\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e3d28d62-0810-4a22-b9ef-7dd2fa1d3721')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e3d28d62-0810-4a22-b9ef-7dd2fa1d3721 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Do you know how much expectation the middle-class working population is having from this budget?\",\n          \"This financial year will end on 31st March.\",\n          \"The interim budget for 2019 will be announced on 1st February.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_num_words(df):\n",
        "    df['number_of_words'] = df['text'].apply(lambda x : len(TextBlob(str(x)).words))\n",
        "    return df\n",
        "add_num_words(df)['number_of_words']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "Ure3qwuletnW",
        "outputId": "dffc58de-74c1-43d1-bec2-51e9562db344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    11\n",
              "1    15\n",
              "2     8\n",
              "3     8\n",
              "Name: number_of_words, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>number_of_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_present(wh_words, df):\n",
        "\n",
        "    # The below line of code will find the intersection between set of tokens of\n",
        "    #  every sentence and the wh_words and will return true if the length of intersection\n",
        "    #  set is non-zero.\n",
        "    df['is_wh_words_present'] = df['text'].apply(lambda x : True if \\\n",
        "                                                 len(set(TextBlob(str(x)).words).intersection(wh_words))>0 else False)\n",
        "    return df\n",
        "\n",
        "wh_words = set(['why', 'who', 'which', 'what', 'where', 'when', 'how'])\n",
        "\n",
        "is_present(wh_words, df)['is_wh_words_present']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "jv6eMIAee0bk",
        "outputId": "f1fcde70-3dd3-4baa-d339-1294b8a06dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    False\n",
              "1     True\n",
              "2    False\n",
              "3    False\n",
              "Name: is_wh_words_present, dtype: bool"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_wh_words_present</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> bool</label>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import nltk\n",
        "\n",
        "nltk.download('tagsets')\n",
        "from nltk.data import load\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNX_JvlofLHB",
        "outputId": "8aea38d3-54f5-433a-ae10-9ba99d8d728f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tagsets():\n",
        "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
        "    return list(tagdict.keys())\n",
        "\n",
        "tag_list = get_tagsets()\n",
        "\n",
        "print(tag_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVfIJpnufPqw",
        "outputId": "b5f40b4c-1525-4952-87a8-2a352ef11de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRP$', 'VBG', 'FW', 'VB', 'POS', \"''\", 'VBP', 'VBN', 'JJ', 'WP', 'VBZ', 'DT', 'RP', '$', 'NN', ')', '(', 'RBR', 'VBD', ',', '.', 'TO', 'LS', 'RB', ':', 'NNS', 'NNP', '``', 'WRB', 'CC', 'PDT', 'RBS', 'PRP', 'CD', 'EX', 'IN', 'WP$', 'MD', 'NNPS', '--', 'JJS', 'JJR', 'SYM', 'UH', 'WDT']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Our Miami restaurant reviews\n",
        "reviews = [\n",
        "    \"Great Cuban food. Amazing Cuban coffee.\",\n",
        "    \"Terrible service. Food was terrible.\",\n",
        "    \"Great service. Amazing food. Great coffee.\"\n",
        "]\n",
        "\n",
        "# Create the Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(reviews)\n",
        "\n",
        "# See the vocabulary\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWSqxx4KilQO",
        "outputId": "924a5fde-925c-461b-80f4-090aabddbbbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['amazing' 'coffee' 'cuban' 'food' 'great' 'service' 'terrible' 'was']\n",
            "\n",
            "Bag of Words Matrix:\n",
            "[[1 1 2 1 1 0 0 0]\n",
            " [0 0 0 1 0 1 2 1]\n",
            " [1 1 0 1 2 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Miami-themed email examples\n",
        "emails = [\n",
        "    # Spam emails\n",
        "    \"WIN FREE cruise to Bahamas from Miami port! Click NOW!!!\",\n",
        "    \"URGENT! Your Miami bank account needs verification! Act fast!\",\n",
        "    \"Congratulations! You won $1000 Miami shopping spree! Claim here!\",\n",
        "    \"FREE tickets to Ultra Music Festival! Limited time offer!\",\n",
        "\n",
        "    # Normal emails\n",
        "    \"Meeting tomorrow at the Brickell office at 2pm\",\n",
        "    \"Can you send me the quarterly report when you have time?\",\n",
        "    \"Let's grab Cuban coffee after the presentation\",\n",
        "    \"The project deadline has been moved to next Friday\"\n",
        "]\n",
        "\n",
        "labels = ['spam', 'spam', 'spam', 'spam',\n",
        "          'normal', 'normal', 'normal', 'normal']\n",
        "\n",
        "# Create BoW\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(emails)\n",
        "\n",
        "# Train classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X, labels)\n",
        "\n",
        "# Test new email\n",
        "new_email = [\"FREE Miami Heat tickets! Click this link now!\"]\n",
        "new_email_bow = vectorizer.transform(new_email)\n",
        "prediction = classifier.predict(new_email_bow)\n",
        "print(f\"Prediction: {prediction[0]}\")  # Output: spam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRBXzYMkjGSo",
        "outputId": "5cdbbcb7-f29d-4e8c-f827-370a6c1add4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorize Miami news articles\n",
        "articles = [\n",
        "    \"The Miami Heat defeated the Lakers in overtime last night at the arena\",\n",
        "    \"New coral restoration project launches in Biscayne Bay this week\",\n",
        "    \"Art Basel Miami Beach announces featured artists for 2024 exhibition\",\n",
        "    \"Hurricane season preparation tips for South Florida residents\",\n",
        "    \"Dolphins quarterback throws three touchdowns in victory\",\n",
        "    \"Climate change impacts on Miami Beach erosion studied by scientists\"\n",
        "]\n",
        "\n",
        "categories = ['sports', 'environment', 'arts', 'weather', 'sports', 'environment']\n",
        "\n",
        "# BoW + Classification\n",
        "vectorizer = CountVectorizer(max_features=50)\n",
        "X = vectorizer.fit_transform(articles)\n",
        "# Now you can train any classifier (SVM, Random Forest, etc.)"
      ],
      "metadata": {
        "id": "pm7M9F1AjM7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Miami restaurant reviews\n",
        "reviews = [\n",
        "    \"Amazing Cuban sandwich! Best in Miami! Will definitely return!\",\n",
        "    \"Terrible service, cold food, never coming back to this place\",\n",
        "    \"Decent food but nothing special, average Miami restaurant\",\n",
        "    \"Horrible experience, worst meal ever, completely disappointed\",\n",
        "    \"Outstanding seafood! Fresh catch! Excellent service! Love it!\",\n",
        "    \"Mediocre at best, expected more from the reviews\"\n",
        "]\n",
        "\n",
        "sentiments = ['positive', 'negative', 'neutral', 'negative', 'positive', 'neutral']\n",
        "\n",
        "# Create BoW\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(reviews)\n",
        "\n",
        "# Train sentiment classifier\n",
        "sentiment_classifier = LogisticRegression()\n",
        "sentiment_classifier.fit(X, sentiments)\n",
        "\n",
        "# Analyze new review\n",
        "new_review = [\"The food was fantastic and service was great!\"]\n",
        "new_bow = vectorizer.transform(new_review)\n",
        "sentiment = sentiment_classifier.predict(new_bow)\n",
        "print(f\"Sentiment: {sentiment[0]}\")  # Output: positive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skmImh8qjRkt",
        "outputId": "2be1b3c2-5fe4-45c0-8648-6ffaaa906588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Miami event descriptions\n",
        "events = [\n",
        "    \"Beach volleyball tournament at South Beach this Saturday\",\n",
        "    \"Sand volleyball competition on South Beach this weekend\",\n",
        "    \"Art gallery opening in Wynwood Arts District Friday night\",\n",
        "    \"New exhibition opens at Wynwood Walls this Friday evening\",\n",
        "    \"Food truck festival at Bayfront Park all weekend\"\n",
        "]\n",
        "\n",
        "# Convert to BoW\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(events)\n",
        "\n",
        "# Calculate similarity\n",
        "similarity_matrix = cosine_similarity(bow_matrix)\n",
        "\n",
        "# Find most similar events to event 0\n",
        "event_idx = 0\n",
        "similarities = similarity_matrix[event_idx]\n",
        "most_similar_idx = similarities.argsort()[-2]  # -1 would be itself\n",
        "\n",
        "print(f\"Event: {events[event_idx]}\")\n",
        "print(f\"Most similar: {events[most_similar_idx]}\")\n",
        "print(f\"Similarity score: {similarities[most_similar_idx]:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqebCkcCjU1h",
        "outputId": "cf6e6d62-30ce-45a7-bad6-861367074f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event: Beach volleyball tournament at South Beach this Saturday\n",
            "Most similar: Sand volleyball competition on South Beach this weekend\n",
            "Similarity score: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_documents(query, documents):\n",
        "    \"\"\"Simple BoW-based search engine\"\"\"\n",
        "    # Combine query with documents\n",
        "    all_text = [query] + documents\n",
        "\n",
        "    # Create BoW\n",
        "    vectorizer = CountVectorizer()\n",
        "    bow_matrix = vectorizer.fit_transform(all_text)\n",
        "\n",
        "    # Calculate similarity between query (index 0) and all docs\n",
        "    query_vector = bow_matrix[0]\n",
        "    doc_vectors = bow_matrix[1:]\n",
        "\n",
        "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
        "\n",
        "    # Rank documents\n",
        "    ranked_idx = similarities.argsort()[::-1]\n",
        "\n",
        "    return [(documents[idx], similarities[idx]) for idx in ranked_idx if similarities[idx] > 0]\n",
        "\n",
        "# Miami business descriptions\n",
        "businesses = [\n",
        "    \"Joe's Stone Crab serves fresh seafood and famous stone crabs\",\n",
        "    \"Versailles Restaurant offers authentic Cuban cuisine and coffee\",\n",
        "    \"Books & Books is an independent bookstore with author events\",\n",
        "    \"Jungle Island features exotic animals and interactive shows\",\n",
        "    \"PÃ©rez Art Museum Miami showcases contemporary and modern art\"\n",
        "]\n",
        "\n",
        "# Search\n",
        "results = search_documents(\"Cuban food restaurant\", businesses)\n",
        "for doc, score in results[:3]:\n",
        "    print(f\"Score: {score:.2f} - {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT81ww9GjiCe",
        "outputId": "0508b77a-f304-4893-c053-c8a8bd616ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.41 - Versailles Restaurant offers authentic Cuban cuisine and coffee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Miami news snippets\n",
        "news_snippets = [\n",
        "    \"Hurricane preparedness workshop scheduled for residents\",\n",
        "    \"Beach erosion concerns grow after recent storms\",\n",
        "    \"Heat win playoff game with Butler scoring 35 points\",\n",
        "    \"Dolphins draft new quarterback in first round\",\n",
        "    \"Art Basel brings international artists to Miami Beach\",\n",
        "    \"Storm surge warnings issued for coastal areas\",\n",
        "    \"Museum opens new contemporary art exhibition\",\n",
        "    \"Basketball team advances to conference finals\"\n",
        "]\n",
        "\n",
        "# Create BoW\n",
        "vectorizer = CountVectorizer(max_features=20, stop_words='english')\n",
        "bow = vectorizer.fit_transform(news_snippets)\n",
        "\n",
        "# Topic modeling\n",
        "lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
        "lda.fit(bow)\n",
        "\n",
        "# Display topics\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words_idx = topic.argsort()[-5:][::-1]\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxtIg0y_jn9V",
        "outputId": "6e115aaa-4388-4459-aa74-1065096bb952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: beach, art, artists, brings, basel\n",
            "Topic 2: new, exhibition, contemporary, draft, dolphins\n",
            "Topic 3: finals, advances, basketball, conference, areas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hotel feedback\n",
        "feedback = [\n",
        "    \"Great location near beach but parking was expensive\",\n",
        "    \"Beautiful beach view and excellent pool area\",\n",
        "    \"Parking was terrible and very expensive\",\n",
        "    \"Love the beach access and pool facilities\",\n",
        "    \"Room was clean but parking situation is horrible\"\n",
        "]\n",
        "\n",
        "# Create BoW focusing on specific aspects\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Include bigrams\n",
        "bow = vectorizer.fit_transform(feedback)\n",
        "\n",
        "# Sum word frequencies\n",
        "word_freq = bow.sum(axis=0).A1\n",
        "word_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Find most mentioned aspects\n",
        "import numpy as np\n",
        "top_indices = word_freq.argsort()[-10:][::-1]\n",
        "\n",
        "print(\"Most mentioned aspects:\")\n",
        "for idx in top_indices:\n",
        "    print(f\"  '{word_names[idx]}': {word_freq[idx]} mentions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY2bVg_1jqX4",
        "outputId": "b1cd2f61-afa5-4222-ac26-3b67df2fc21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most mentioned aspects:\n",
            "  'was': 3 mentions\n",
            "  'parking': 3 mentions\n",
            "  'and': 3 mentions\n",
            "  'beach': 3 mentions\n",
            "  'pool': 2 mentions\n",
            "  'parking was': 2 mentions\n",
            "  'but parking': 2 mentions\n",
            "  'expensive': 2 mentions\n",
            "  'but': 2 mentions\n",
            "  'was clean': 1 mentions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Miami's multilingual environment\n",
        "texts = [\n",
        "    \"Welcome to Miami Beach, enjoy your stay!\",  # English\n",
        "    \"Bienvenido a Miami Beach, disfruta tu estancia!\",  # Spanish\n",
        "    \"The best Cuban coffee in all of Miami\",  # English\n",
        "    \"El mejor cafÃ© cubano de todo Miami\",  # Spanish\n",
        "    \"Bienvenue Ã  Miami Beach!\"  # French\n",
        "]\n",
        "\n",
        "languages = ['english', 'spanish', 'english', 'spanish', 'french']\n",
        "\n",
        "# Character-level BoW can help detect languages\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))\n",
        "X = char_vectorizer.fit_transform(texts)\n",
        "\n",
        "# Train language detector\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "lang_detector = MultinomialNB()\n",
        "lang_detector.fit(X, languages)\n",
        "\n",
        "# Detect language\n",
        "new_text = [\"Hola, dÃ³nde estÃ¡ la playa?\"]\n",
        "new_bow = char_vectorizer.transform(new_text)\n",
        "detected = lang_detector.predict(new_bow)\n",
        "print(f\"Detected language: {detected[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg-gifcIjvkl",
        "outputId": "3b0ddfa3-c71f-49b3-dbf4-f24aa4f22450"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: spanish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Activity 2.01: Extracting Top Keywords from the News Article\n",
        "\n",
        "In this activity, you will extract the most frequently occurring keywords from a sample news article using Python and the Natural Language Toolkit (NLTK).\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Basic understanding of Python programming.\n",
        "- An environment to run Python code (like Jupyter Notebook or Google Colab).\n",
        "\n",
        "### Data\n",
        "\n",
        "The news article used in this activity is available at the following link: [news_article.txt](https://github.com/fenago/natural-language-processing-workshop/blob/master/Lab02/data/news_article.txt).\n",
        "\n",
        "### Steps to Follow\n",
        "\n",
        "1. **Set Up Your Environment**:\n",
        "   - Open Jupyter Notebook or Google Colab.\n",
        "   - Ensure Python is installed along with NLTK. Install NLTK if not already installed using `!pip install nltk`.\n",
        "\n",
        "2. **Import Necessary Libraries**:\n",
        "   - Import `nltk` and other necessary Python libraries.\n",
        "\n",
        "3. **Define Helper Functions**:\n",
        "   - Create functions to load the text file, convert text to lowercase, tokenize the text, remove stop words, perform stemming, and calculate word frequencies.\n",
        "\n",
        "4. **Load the News Article**:\n",
        "   - Use Python's file handling methods to load `news_article.txt` into a string.\n",
        "\n",
        "5. **Preprocess the Text**:\n",
        "   - Convert the text to lowercase.\n",
        "   - Tokenize the text using a whitespace tokenizer.\n",
        "   - Remove stop words from the tokens.\n",
        "   - Perform stemming on the remaining tokens.\n",
        "\n",
        "6. **Calculate Word Frequencies**:\n",
        "   - Count the frequency of each word after stemming.\n",
        "   - Display the most frequent keywords.\n",
        "\n",
        "### Challenge for Students\n",
        "\n",
        "Now that you've learned how to extract keywords from a news article, challenge yourself by applying these techniques to a different dataset. Here's what you can do:\n",
        "\n",
        "- **Find a Unique Dataset**: Select a text dataset of your interest. This could be another news article, a blog post, or any textual data.\n",
        "- **Implement the Keyword Extraction Process**: Apply the steps you've learned in this activity to your dataset. This includes text preprocessing, tokenization, stop word removal, stemming, and frequency analysis.\n",
        "- **Analyze Your Results**: Look at the most frequent keywords in your dataset. Do they give you insights into the main themes or topics of the text?\n",
        "\n",
        "**Contextualize Your Learning**: Reflect on how this process could be useful in real-world applications like search engine optimization, content analysis, or summarizing information.\n"
      ],
      "metadata": {
        "id": "A3UA0n3bkRCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for the news_article.txt - just create a text file and put this into the contents:\n",
        "\n",
        "Ever since the populist Law and Justice (pis) party took power in 2015, Adam Bodnar, Polandâ€™s\n",
        " human-rights ombudsman, has been warning against its relentless efforts to get control of the\n",
        " courts. To illustrate the danger, he uses an expression from communist times: lex telefonica.\n",
        " In the Polish Peopleâ€™s Republic, verdicts were routinely dictated by a phone call from an\n",
        " apparatchik at party headquarters. Todayâ€™s government has more subtle techniques,\n",
        " but the goal is the same, Mr Bodnar says: â€œIf a judge has a case on his desk with some\n",
        " political importance, he should be afraid.â€\n",
        "\n",
        "The European Commission is worried, too. It accuses pis of violating Polandâ€™s commitments\n",
        "to the rule of law under the European Unionâ€™s founding treaty. In 2017 the commission took\n",
        "Poland to the European Court of Justice (ecj) over laws that gave politicians control over\n",
        "appointing judges. (For example, they lowered judgesâ€™ retirement age while letting the justice\n",
        " minister pick whom to exempt.) The ecj ruled against the Poles, who had in the meantime\n",
        " scrapped some of the measures."
      ],
      "metadata": {
        "id": "Q89dgrN3ku31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Activity 2.02: Text Visualization\n",
        "\n",
        "In this activity, you will create a word cloud for the 50 most frequent words in a dataset. The dataset consists of random sentences that need to be cleaned and analyzed to identify frequently occurring words.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Basic understanding of Python programming.\n",
        "- Familiarity with text processing and visualization libraries in Python.\n",
        "\n",
        "### Data\n",
        "\n",
        "The dataset used in this activity is available at the following link: [text_corpus.txt](https://github.com/fenago/natural-language-processing-workshop/blob/master/Lab02/data/text_corpus.txt\n",
        ").\n",
        "\n",
        "### Steps to Follow\n",
        "\n",
        "1. **Import Necessary Libraries**:\n",
        "   - Import libraries required for data fetching, text processing, and visualization (like `pandas`, `nltk`, `matplotlib`, `wordcloud`, etc.).\n",
        "\n",
        "2. **Fetch the Dataset**:\n",
        "   - Retrieve the `text_corpus.txt` file and load its contents.\n",
        "\n",
        "3. **Preprocess the Text**:\n",
        "   - Perform text cleaning to remove unwanted characters and formats.\n",
        "   - Tokenize the text.\n",
        "   - Apply lemmatization to convert words to their base form.\n",
        "\n",
        "4. **Identify Top 50 Words**:\n",
        "   - Calculate the frequency of each word in the cleaned dataset.\n",
        "   - Create a set of the top 50 most frequent words along with their frequencies.\n",
        "\n",
        "5. **Create a Word Cloud**:\n",
        "   - Use the word cloud library to visualize the top 50 words.\n",
        "   - Customize the word cloud's appearance as needed.\n",
        "\n",
        "6. **Analyze the Word Cloud**:\n",
        "   - Compare the word cloud with the calculated word frequencies.\n",
        "   - Justify the representation of words in the word cloud based on their frequencies.\n",
        "\n",
        "### Challenge for Students\n",
        "\n",
        "Now that you have created a word cloud for a given dataset, try extending your skills with these tasks:\n",
        "\n",
        "- **Use a Different Dataset**: Find another text dataset that interests you. It could be a collection of social media posts, reviews, or any other textual content.\n",
        "- **Apply Enhanced Text Processing**: Experiment with different preprocessing techniques like stop word removal, n-grams, or POS tagging.\n",
        "- **Visualize Your Findings**: Create a word cloud for your chosen dataset. How does the word cloud reflect the key themes or sentiments in the data?\n",
        "- **Draw Insights**: Reflect on how word clouds can aid in quick data analysis, highlighting key areas for deeper exploration.\n",
        "\n",
        "**Explore Further**: Consider how word clouds can be used in areas like marketing analysis, sentiment analysis, or summarizing large volumes of text.\n"
      ],
      "metadata": {
        "id": "smnw-hnrlbTE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}